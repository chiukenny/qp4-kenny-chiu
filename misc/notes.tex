\documentclass[10pt]{article}
\input{../document/report/header}
\input{../document/report/defs}


\begin{document}

\tableofcontents


\newpage


\section{Optimal Iterative Sketching with the Subsampled Randomized Hadamard Transform}

Based on \citep{Lacotte:2020}.
\\

The performance of iterative Hessian sketch (IHS) has only been studied empirically in existing literature. \citet{Lacotte:2020} show that for IHS with random matrices projected via refreshed (\iid) truncated Haar matrices or subsampled randomized Hadamard transform (SRHT), the limiting rate of convergence is expected to be better than that of IHS with Gaussian random projections. Their other theoretical contributions include a closed form optimal (limiting) step size for IHS with Haar sketches, showing that momentum does not improve performance of IHS with refreshed Haar sketches, and an explicit formula for the second inverse moment of Haar sketches.

\subsection{Background}

\subsubsection{Problem and method}

Consider overdetermined least-squares problems of the form
\[
\bfb^* = \argmin_{\bfb\in\bbR^d}\left\{f(\bfb) = \frac{1}{2}\|\bfX\bfb-\bfy\|^2\right\}
\]
where $\bfX\in\bbR^{n\times d}$ is a given data matrix with $n\geq d$ and $\mathrm{rank}(\bfX)=d$ and $\bfy\in\bbR^n$ is a vector of observations. Iterative Hessian sketch is one iterative method for solving the problem where given step sizes $\{\alpha_t\}$ and momentum $\{\beta_t\}$, the solutions are iteratively updated by
\[
\bfb_{t+1} = \bfb_t - \alpha_tH_t^{-1}\nabla f(\bfb_t)+\beta_t(\bfb_t-\bfb_{t-1}) \;.
\]
The matrix $H_t$ is an approximation of the Hessian $H=\bfX^\T\bfX$ and is given by $H_t=\bfX^\T \bfS_t^\T \bfS_t\bfX$ where $\bfS_0,\ldots,\bfS_t,\ldots$ are refreshed (\iid) $m\times n$ sketching (random) matrices with $m\ll n$. The types of sketches discussed by \citet{Lacotte:2020} include
\begin{enumerate}

\item
Gaussian sketches where $(\bfS_t)_{ij}\overset{\text{\iid}}{\sim} N(0,m^{-1})$. Computing the matrix product $\bfS\bfX$ is $O(mnd)$ in general, which is larger than the cost of $O(nd^2)$ for direct method solvers when $m\geq d$.

\item
truncated Haar sketches using Haar matrices $\bfS_t$ where the rows are orthonormal (\todo: other qualifications? Truncated?). Generating the matrix requires $O(nm^2)$ using a Gram-Schmidt procedure which is larger than $O(nd^2)$.

\item
subsampled randomized Hadamard transform where the sketch $\bfS\bfX$ can be obtained in $O(nd\log m)$ time. Like other orthogonal embeddings, the performance tends to be better than random projections with \iid entries.

\end{enumerate}


\subsubsection{Random matrix theory and tools}

Let $\{\bfM_n\}_n$ be a sequence of $n\times n$ Hermitian random matrices. The empirical spectral distribution (\esd) of $\bfM_n$ is the CDF of its eigenvalues $\lambda_1,\ldots,\lambda_n$ given by $F_{\bfM_n}(x)=\frac{1}{n}\sum_{j=1}^n\mathbbm{1}[\lambda_j\leq x]$ for $x\in\bbR$. The eigenvalues are random and so $F_{\bfM_n}$ is also random. The \esd $F_{\bfM_n}$ converges weakly to the limiting spectral distribution (\lsd) of $\bfM_n$ as $n\rightarrow\infty$.
\\

For a probability measure $\mu$ with support on $[0,\infty)$, its Stieltjes transform is defined over the complex space complementary to the support of $\mu$ as
\[
m_\mu(z) = \int \frac{1}{x-z}\mu(dx) \;.
\]
The $S$-transform of $\mu$ is unique under certain conditions and is defined as the solution to the equation
\[
m_\mu\left(\frac{z+1}{zS_\mu(z)}\right) + zS_\mu(z) = 0\;.
\]

The Marchenko-Pastur theorem says that for a $m\times d$ matrix $\bfS$ where $(\bfS)_{ij}\overset{\text{\iid}}{\sim}N(0,m^{-1})$, then as $m,d\rightarrow\infty$ with $\frac{m}{d}\rightarrow\rho\in(0,1)$, $\bfS^\T\bfS$ has \lsd $F_\rho$ with a Stieltjes transform that is the unique solution of a certain fixed point equation and with a density given by
\[
\mu_\rho(x) = \frac{\sqrt{(1+\sqrt{\rho})^2-x)_+(x-(1-\sqrt{\rho})^2)_+}}{2\pi\rho x}
\]
where $y_+=\max\{0,y\}$.


\subsubsection{Other notation}

Define the aspect ratios $\gamma = \lim_{n,d\rightarrow\infty}\frac{d}{n}\in(0,1)$, $\xi = \lim_{n,m\rightarrow\infty}\frac{m}{n}\in(\gamma,1)$ and $\rho_g=\frac{\gamma}{\xi}\in(0,1)$ where subscript $g$ refers to Gaussians and $h$ refers to Haar or Hadamard. For a sequence $\{\bfb_t\}$, denote the error vector $\Delta_t = \bfU^\T\bfX(\bfb_t-\bfb^*)$ where $\bfU$ is the $n\times d$ matrix of left singular vectors of $\bfX$. Note that $\|\Delta_t\|^2 = \|\bfX(\bfb_t-\bfb^*)\|^2$.


\subsection{Sketching with Haar matrices}

Theorem~3.1 (Optimal IHS with Haar sketches): for refreshed Haar matrices $\{\bfS_t\}$, step sizes $\alpha_t=\frac{\theta_{1,h}}{\theta_{2,h}}$ (defined in Lemma~3.2) and momentum parameters $\beta_t=0$, the sequence of error vectors $\{\Delta_t\}$ satisfies
\[
\rho_h = \left(\lim_{n\rightarrow\infty}\frac{\E\|\Delta_t\|^2}{\|\Delta_0\|^2}\right)^\frac{1}{t} = \rho_g\cdot\frac{\xi(1-\xi)}{\gamma^2+\xi-2\xi\gamma} \;.
\]
For any step sizes $\{a_t\}$ and momentum parameters $\{\beta_t\}$,
\[
\rho_h \leq \liminf_{t\rightarrow\infty}\left(\lim_{n\rightarrow\infty}\frac{\E\|\Delta_t\|^2}{\|\Delta_0\|^2}\right)^\frac{1}{t} \;,
\]
i.e., $\rho_h$ is the optimal rate for Haar embeddings.
\\

Theorem~3.1 says that using the optimal parameters (which has closed forms), the rate at any time step $t\geq1$ is given by
\[
\rho_h^t = \lim_{n\rightarrow\infty}\frac{\E\|\Delta_t\|^2}{\|\Delta_0\|^2}
\]
with $\rho_h<\rho_g$. Momentum also does not provide benefits.
\\

Lemma~3.2 (First two inverse moments of Haar sketches): let $\bfS$ be a $m\times n$ Haar matrix, $\bfU$ a $n\times d$ deterministic matrix with orthonormal columns. Then
\begin{align*}
\theta_{1,h} &= \lim_{n\rightarrow\infty} \frac{1}{d} \mathrm{trace}\left(\E\left[(\bfU^\T\bfS^\T\bfS\bfU)^{-1}\right]\right) = \frac{1-\gamma}{\xi-\gamma} \\
\theta_{2,h} &= \lim_{n\rightarrow\infty} \frac{1}{d} \mathrm{trace}\left(\E\left[(\bfU^\T\bfS^\T\bfS\bfU)^{-2}\right]\right) = \frac{(1-\gamma)(\gamma^2+\xi-2\gamma\xi)}{(\xi-\gamma)^3}
\end{align*}
(Note that $\theta_{i,h}$ is the average of the eigenvalues of $\bfU^\T\bfS^\T\bfS\bfU$ to the power of $-i$.)
\\

\citet{Lacotte:2020} show that as the sketch size $m$ increases relative to $n$, the convergence ratio of Haar sketches versus Gaussian projections scales as $\frac{\rho_h}{\rho_g}\approx (1-\xi)$.


\subsection{Sketching with SRHT matrices}

\citet{Lacotte:2020} consider a version of SRHT where the transform $\bfX\mapsto\bfS\bfX$ first randomly permutes the rows of $\bfX$ before applying the classical transform, i.e., $\bfS=\frac{1}{\sqrt{n}}\bfB\bfH_n\bfD\bfP$ where $\bfB$ is a $n\times n$ diagonal matrix of \iid Bernoulli random variables with success probability $\frac{m}{n}$, $\bfD$ is a $n\times n$ diagonal matrix of \iid sign random variables with uniform probability, and $\bfP$ is a $n\times n$ uniformly distributed permutation matrix. $\bfH_n$ is the $n\times n$ Walsh-Hadamard matrix where for $n=2^p$ for $p\geq1$, the matrix is defined recursively as
\[
\bfH_n = \begin{bmatrix}
\bfH_\frac{n}{2} & \bfH_\frac{n}{2} \\
\bfH_\frac{n}{2} & -\bfH_\frac{n}{2}
\end{bmatrix}
\]
with $\bfH_1=1$. Before applying the transformation to $\bfX$, the zero rows of $\bfS$ are discarded and so $\bfS$ is a $M\times n$ orthogonal matrix with $M\sim\mathrm{Binomial}(\frac{m}{n},n)$, and $\frac{M}{n}\rightarrow\xi$ as $n\rightarrow\infty$. Note that $\bfS$ is still referred to as a $m\times n$ SRHT matrix.
\\

Theorem~4.1 (IHS with SRHT sketches): suppose that $\bfb_0$ is random and that the error vector $\Delta_0$ satisfies $\E\left[\Delta_0\Delta_0^\T\right]=d^{-1}\bfI_d$. Then for refreshed SRHT matrices $\{\bfS_t\}$, step sizes $\alpha_t=\frac{\theta_{1,h}}{\theta_{2,h}}$ and momentum parameters $\beta_t=0$,  the sequence $\{\Delta_t\}$ satisfies
\[
\rho_s = \left(\lim_{n\rightarrow\infty}\frac{\E\left[\|\Delta_t\|^2\right]}{\E\left[\|\Delta_0\|^2\right]}\right)^\frac{1}{t} = \rho_g \cdot \frac{\xi(1-\xi)}{\gamma^2+\xi-2\xi\gamma} = \rho_h \;.
\]

The additional initialization condition can be satisfied by picking $\bfb_0$ uniformly from the unit $d$-sphere $\bfS^{d-1}$ and applying a random signed permutation and scaling to the columns of $\bfX$ (\todo). The case of general $\Delta_0$ or momentum $\beta_t\neq 0$ has not yet been explored.
\\

Theorem~4.2 (Upper bound of SRHT error): for any $\bfb_0$ with refreshed SRHT matrices $\{\bfS_t\}$, step sizes $\alpha_t=\frac{\theta_{1,h}}{\theta_{2,h}}$ and momentum parameters $\beta_t=0$, the sequence of error vectors $\{\Delta_t\}$ satisfies
\[
\limsup_{n\rightarrow\infty} \left(\frac{\E\left[\|\Delta_t\|\right]^2}{d\E\left[\|\Delta_0\|\right]^2}\right)^\frac{1}{t} \leq \rho_h \;.
\]
(Weaker by a factor of $d$, but is negligible for large $t$.)
\\

Lemma~4.3 (First two inverse moments of SRHT sketches): let $\bfS$ be a $m\times n$ SRHT matrix, $\bfS_h$ a $m\times n$ Haar matrix and $\bfU$ a $n\times d$ deterministic matrix with orthonormal columns. Then $\bfU^\T\bfS^\T\bfS\bfU$ and $\bfU^\T\bfS_h^T\bfS_h\bfU$ have the same limiting spectral distribution and therefore the same two inverse moments.


\subsection{Complexity analysis}

\citet{Lacotte:2020} compare the (asymptotic) complexity of IHS with SRHT embeddings to that of the standard pre-conditioned conjugate gradient method. The latter uses a sketch $\bfS\bfX$ to compute a pre-conditioning matrix $\bfP$ such that $\bfX\bfP^{-1}$ has a small condition number and solves the least squares problem $\min_\bfb\|\bfX\bfP^{-1}\bfb-\bfy\|^2$. The sketch size is prescribed to be $m\approx d\log d$, and the complexity to achieve $\|\Delta_t\|^2\leq \epsilon$ scales as $C_c\asymp nd\log d+d^3\log d+nd\log\varepsilon^{-1}$ (cost of forming $\bfS\bfX$, factoring, and per-iteration cost times number of iterations, respectively). For IHS with SRHT, we can take $m\approx d$ which results in complexity $C_n\asymp (nd\log d+d^3+nd)\log\varepsilon^{-1}$. Treating $\log\varepsilon^{-1}$ as constant independent of the dimensions, $\frac{C_n}{C_c}\asymp \frac{1}{\log d}$ as $n,m,d\rightarrow\infty$.


\subsection{Numerical simulations}

The main results of the numerical simulations by \citet{Lacotte:2020} (involving ill-conditioned matrices) include:
\begin{itemize}

\item
IHS with refreshed Haar/SRHT embeddings (using optimal step size from Theorem~4.1 and finite sample approximations of $\xi$ and $\gamma$) converge faster than IHS with refreshed Gaussian embeddings (using parameters $\alpha_t$ and $\beta_t$ derived from previous work).

\item
Haar embeddings and SRHT embeddings perform similarly (though SRHT has a computational advantage).

\item
IHS with refreshed SRHT embeddings every iteration converge faster than the pre-conditioned conjugate gradient method. IHS with sketches refreshed at lower frequencies converge slower.

\end{itemize}


\newpage

\bibliographystyle{plainnat}
\bibliography{../references/qp}

\end{document}
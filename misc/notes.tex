\documentclass[10pt]{article}
\input{../document/report/header}
\input{../document/report/defs}


\begin{document}

\tableofcontents


\newpage


\section{Optimal Iterative Sketching with the Subsampled Randomized Hadamard Transform}

Based on \citep{Lacotte:2020}.
\\

The performance of iterative Hessian sketch (IHS) has only been studied empirically in existing literature. \citet{Lacotte:2020} show that for IHS with random matrices projected via refreshed (\iid) truncated Haar matrices or subsampled randomized Hadamard transform (SRHT), the limiting rate of convergence is expected to be better than that of IHS with Gaussian random projections. Their other theoretical contributions include a closed form optimal (limiting) step size for IHS with Haar sketches, showing that momentum does not improve performance of IHS with refreshed Haar sketches, and an explicit formula for the second inverse moment of Haar sketches.

\subsection{Background}

\subsubsection{Problem and method}

Consider overdetermined least-squares problems of the form
\[
\bfb^* = \argmin_{\bfb\in\bbR^d}\left\{f(\bfb) = \frac{1}{2}\|\bfX\bfb-\bfy\|^2\right\}
\]
where $\bfX\in\bbR^{n\times d}$ is a given data matrix with $n\geq d$ and $\mathrm{rank}(\bfX)=d$ and $\bfy\in\bbR^n$ is a vector of observations. Iterative Hessian sketch is one iterative method for solving the problem where given step sizes $\{\alpha_t\}$ and momentum $\{\beta_t\}$, the solutions are iteratively updated by
\[
\bfb_{t+1} = \bfb_t - \alpha_tH_t^{-1}\nabla f(\bfb_t)+\beta_t(\bfb_t-\bfb_{t-1}) \;.
\]
The matrix $H_t$ is an approximation of the Hessian $H=\bfX^\T\bfX$ and is given by $H_t=\bfX^\T \bfS_t^\T \bfS_t\bfX$ where $\bfS_0,\ldots,\bfS_t,\ldots$ are refreshed (\iid) $m\times n$ sketching (random) matrices with $m\ll n$. The types of sketches discussed by \citet{Lacotte:2020} include
\begin{enumerate}

\item
Gaussian sketches where $(\bfS_t)_{ij}\overset{\text{\iid}}{\sim} N(0,m^{-1})$. Computing the matrix product $\bfS\bfX$ is $O(mnd)$ in general, which is larger than the cost of $O(nd^2)$ for direct method solvers when $m\geq d$.

\item
truncated Haar sketches using Haar matrices $\bfS_t$ where the rows are orthonormal. Generating the matrix requires $O(nm^2)$ using a Gram-Schmidt procedure which is larger than $O(nd^2)$.

\item
subsampled randomized Hadamard transform where the sketch $\bfS\bfX$ can be obtained in $O(nd\log m)$ time. Like other orthogonal embeddings, the performance tends to be better than random projections with \iid entries.

\end{enumerate}


\subsubsection{Random matrix theory and tools}

Let $\{\bfM_n\}_n$ be a sequence of $n\times n$ Hermitian random matrices. The empirical spectral distribution (\esd) of $\bfM_n$ is the CDF of its eigenvalues $\lambda_1,\ldots,\lambda_n$ given by $F_{\bfM_n}(x)=\frac{1}{n}\sum_{j=1}^n\mathbbm{1}[\lambda_j\leq x]$ for $x\in\bbR$. The eigenvalues are random and so $F_{\bfM_n}$ is also random. The \esd $F_{\bfM_n}$ converges weakly to the limiting spectral distribution (\lsd) of $\bfM_n$ as $n\rightarrow\infty$.
\\

For a probability measure $\mu$ with support on $[0,\infty)$, its Stieltjes transform is defined over the complex space of $z$ outside the support of $\mu$ as
\[
m_\mu(z) = \int \frac{1}{x-z}\mu(dx) \;.
\]
The $S$-transform of $\mu$ is unique under certain conditions and is defined as the solution to the equation
\[
m_\mu\left(\frac{z+1}{zS_\mu(z)}\right) + zS_\mu(z) = 0\;.
\]
The $\eta$-transform is an alternative form of the Stieltjes transform defined for $z\in\bbC\backslash\bbR^-$ and given by
\[
\eta_\mu(z) = \int\frac{1}{1+zx}\mu(dx) = \frac{1}{z}m_\mu\left(-\frac{1}{z}\right) \;.
\]

The Marchenko-Pastur theorem says that for a $m\times d$ matrix $\bfS$ where $(\bfS)_{ij}\overset{\text{\iid}}{\sim}N(0,m^{-1})$, then as $m,d\rightarrow\infty$ with $\frac{m}{d}\rightarrow\rho\in(0,1)$, $\bfS^\T\bfS$ has \lsd $F_\rho$ with a Stieltjes transform that is the unique solution of a certain fixed point equation and with a density given by
\[
\mu_\rho(x) = \frac{\sqrt{(1+\sqrt{\rho})^2-x)_+(x-(1-\sqrt{\rho})^2)_+}}{2\pi\rho x}
\]
where $y_+=\max\{0,y\}$.
\\

For a random matrix $\bfX_n$ in algebra $\calA_n$ of $n\times n$ matrices, the normalized trace operator
\[
\tau_n(\bfX_n) = \frac{1}{n}\E\left[\mathrm{trace}(\bfX_n)\right]
\]
is a linear functional (and is analogous to expectations with scalar random variables; see Terry Tao's notes on free probability).


\subsubsection{Other notation}

Define the aspect ratios $\gamma = \lim_{n,d\rightarrow\infty}\frac{d}{n}\in(0,1)$, $\xi = \lim_{n,m\rightarrow\infty}\frac{m}{n}\in(\gamma,1)$ and $\rho_g=\frac{\gamma}{\xi}\in(0,1)$ where subscript $g$ refers to Gaussians and $h$ refers to Haar or Hadamard. For a sequence $\{\bfb_t\}$, denote the error vector $\Delta_t = \bfU^\T\bfX(\bfb_t-\bfb^*)$ where $\bfU$ is the $n\times d$ matrix of left singular vectors of $\bfX$. Note that $\|\Delta_t\|^2 = \|\bfX(\bfb_t-\bfb^*)\|^2$.


\subsection{Sketching with Haar matrices}

Theorem~3.1 (Optimal IHS with Haar sketches): for refreshed Haar matrices $\{\bfS_t\}$, step sizes $\alpha_t=\frac{\theta_{1,h}}{\theta_{2,h}}$ (defined in Lemma~3.2) and momentum parameters $\beta_t=0$, the sequence of error vectors $\{\Delta_t\}$ satisfies
\[
\rho_h = \left(\lim_{n\rightarrow\infty}\frac{\E\|\Delta_t\|^2}{\|\Delta_0\|^2}\right)^\frac{1}{t} = \rho_g\cdot\frac{\xi(1-\xi)}{\gamma^2+\xi-2\xi\gamma} \;.
\]
For any step sizes $\{a_t\}$ and momentum parameters $\{\beta_t\}$,
\[
\rho_h \leq \liminf_{t\rightarrow\infty}\left(\lim_{n\rightarrow\infty}\frac{\E\|\Delta_t\|^2}{\|\Delta_0\|^2}\right)^\frac{1}{t} \;,
\]
i.e., $\rho_h$ is the optimal rate for Haar embeddings.
\\

Theorem~3.1 says that using the optimal parameters (which has closed forms), the rate at any time step $t\geq1$ is given by
\[
\rho_h^t = \lim_{n\rightarrow\infty}\frac{\E\|\Delta_t\|^2}{\|\Delta_0\|^2}
\]
with $\rho_h<\rho_g$. Momentum also does not provide benefits.
\\

Lemma~3.2 (First two inverse moments of Haar sketches): let $\bfS$ be a $m\times n$ Haar matrix, $\bfU$ a $n\times d$ deterministic matrix with orthonormal columns. Then
\begin{align*}
\theta_{1,h} &= \lim_{n\rightarrow\infty} \frac{1}{d} \mathrm{trace}\left(\E\left[(\bfU^\T\bfS^\T\bfS\bfU)^{-1}\right]\right) = \frac{1-\gamma}{\xi-\gamma} \\
\theta_{2,h} &= \lim_{n\rightarrow\infty} \frac{1}{d} \mathrm{trace}\left(\E\left[(\bfU^\T\bfS^\T\bfS\bfU)^{-2}\right]\right) = \frac{(1-\gamma)(\gamma^2+\xi-2\gamma\xi)}{(\xi-\gamma)^3}
\end{align*}
(Note that $\theta_{i,h}$ is the average of the eigenvalues of $\bfU^\T\bfS^\T\bfS\bfU$ to the power of $-i$.)
\\

\citet{Lacotte:2020} show that as the sketch size $m$ increases relative to $n$, the convergence ratio of Haar sketches versus Gaussian projections scales as $\frac{\rho_h}{\rho_g}\approx (1-\xi)$.


\subsection{Sketching with SRHT matrices}

\citet{Lacotte:2020} consider a version of SRHT where the transform $\bfX\mapsto\bfS\bfX$ first randomly permutes the rows of $\bfX$ before applying the classical transform, i.e., $\bfS=\frac{1}{\sqrt{n}}\bfB\bfH_n\bfD\bfP$ where $\bfB$ is a $n\times n$ diagonal matrix of \iid Bernoulli random variables with success probability $\frac{m}{n}$, $\bfD$ is a $n\times n$ diagonal matrix of \iid sign random variables with uniform probability, and $\bfP$ is a $n\times n$ uniformly distributed permutation matrix. $\bfH_n$ is the $n\times n$ Walsh-Hadamard matrix where for $n=2^p$ for $p\geq1$, the matrix is defined recursively as
\[
\bfH_n = \begin{bmatrix}
\bfH_\frac{n}{2} & \bfH_\frac{n}{2} \\
\bfH_\frac{n}{2} & -\bfH_\frac{n}{2}
\end{bmatrix}
\]
with $\bfH_1=1$. Before applying the transformation to $\bfX$, the zero rows of $\bfS$ are discarded and so $\bfS$ is a $M\times n$ orthogonal matrix with $M\sim\mathrm{Binomial}(\frac{m}{n},n)$, and $\frac{M}{n}\rightarrow\xi$ as $n\rightarrow\infty$. Note that $\bfS$ is still referred to as a $m\times n$ SRHT matrix.
\\

Theorem~4.1 (IHS with SRHT sketches): suppose that $\bfb_0$ is random and that the error vector $\Delta_0$ satisfies $\E\left[\Delta_0\Delta_0^\T\right]=d^{-1}\bfI_d$. Then for refreshed SRHT matrices $\{\bfS_t\}$, step sizes $\alpha_t=\frac{\theta_{1,h}}{\theta_{2,h}}$ and momentum parameters $\beta_t=0$,  the sequence $\{\Delta_t\}$ satisfies
\[
\rho_s = \left(\lim_{n\rightarrow\infty}\frac{\E\left[\|\Delta_t\|^2\right]}{\E\left[\|\Delta_0\|^2\right]}\right)^\frac{1}{t} = \rho_g \cdot \frac{\xi(1-\xi)}{\gamma^2+\xi-2\xi\gamma} = \rho_h \;.
\]

The additional initialization condition can be satisfied by picking $\bfb_0$ uniformly from the unit $d$-sphere $\bfS^{d-1}$ and applying a random signed permutation and scaling to the columns of $\bfX$ (\todo). The case of general $\Delta_0$ or momentum $\beta_t\neq 0$ has not yet been explored.
\\

Theorem~4.2 (Upper bound of SRHT error): for any $\bfb_0$ with refreshed SRHT matrices $\{\bfS_t\}$, step sizes $\alpha_t=\frac{\theta_{1,h}}{\theta_{2,h}}$ and momentum parameters $\beta_t=0$, the sequence of error vectors $\{\Delta_t\}$ satisfies
\[
\limsup_{n\rightarrow\infty} \left(\frac{\E\left[\|\Delta_t\|\right]^2}{d\E\left[\|\Delta_0\|\right]^2}\right)^\frac{1}{t} \leq \rho_h \;.
\]
(Weaker by a factor of $d$, but is negligible for large $t$.)
\\

Lemma~4.3 (First two inverse moments of SRHT sketches): let $\bfS$ be a $m\times n$ SRHT matrix, $\bfS_h$ a $m\times n$ Haar matrix and $\bfU$ a $n\times d$ deterministic matrix with orthonormal columns. Then $\bfU^\T\bfS^\T\bfS\bfU$ and $\bfU^\T\bfS_h^T\bfS_h\bfU$ have the same limiting spectral distribution and therefore the same two inverse moments.


\subsection{Complexity analysis}

\citet{Lacotte:2020} compare the (asymptotic) complexity of IHS with SRHT embeddings to that of the standard pre-conditioned conjugate gradient method. The latter uses a sketch $\bfS\bfX$ to compute a pre-conditioning matrix $\bfP$ such that $\bfX\bfP^{-1}$ has a small condition number and solves the least squares problem $\min_\bfb\|\bfX\bfP^{-1}\bfb-\bfy\|^2$. The sketch size is prescribed to be $m\approx d\log d$, and the complexity to achieve $\|\Delta_t\|^2\leq \epsilon$ scales as $C_c\asymp nd\log d+d^3\log d+nd\log\varepsilon^{-1}$ (cost of forming $\bfS\bfX$, factoring, and per-iteration cost times number of iterations, respectively). For IHS with SRHT, we can take $m\approx d$ which results in complexity $C_n\asymp (nd\log d+d^3+nd)\log\varepsilon^{-1}$. Treating $\log\varepsilon^{-1}$ as constant independent of the dimensions, $\frac{C_n}{C_c}\asymp \frac{1}{\log d}$ as $n,m,d\rightarrow\infty$.


\subsection{Numerical simulations}

The main results of the numerical simulations by \citet{Lacotte:2020} (involving ill-conditioned matrices) include:
\begin{itemize}

\item
IHS with refreshed Haar/SRHT embeddings (using optimal step size from Theorem~4.1 and finite sample approximations of $\xi$ and $\gamma$) converge faster than IHS with refreshed Gaussian embeddings (using parameters $\alpha_t$ and $\beta_t$ derived from previous work).

\item
Haar embeddings and SRHT embeddings perform similarly (though SRHT has a computational advantage).

\item
IHS with refreshed SRHT embeddings every iteration converge faster than the pre-conditioned conjugate gradient method. IHS with sketches refreshed at lower frequencies converge slower.

\end{itemize}


\subsection{Appendix}

\subsubsection{B.2}

\begin{itemize}

\item
Distribution $F_\gamma$ with density $\gamma\delta_1+(1-\gamma)\delta_0$

\item
Distribution $F_\xi$ with density $\xi\delta_1+(1-\xi)\delta_0$

\item
System of equations
\begin{align*}
\eta_C(z) &= \int \frac{1}{z\gamma(z)x+1}F_\xi(dx) \\
&= \frac{\xi}{z\gamma(z)+1} + 1-\xi \\
\gamma(z) &= \int\frac{x}{\eta_C(z)+z\delta(z)x}F_\gamma(dx) \\
&= \frac{\gamma}{\eta_C(z) + z\delta(z)} \\
\delta(z) &= \int\frac{x}{z\gamma(z)x+1}F_\xi(dx) \\
&= \frac{\xi}{z\gamma(z)+1}
\end{align*}
\todo
\begin{align*}
\eta_C(z) &= (1-\gamma) + \frac{\gamma}{(1+z\left(1+\frac{\xi-1}{\eta_C(z)}\right)}
\end{align*}

\end{itemize}


\subsection{Code}

\href{https://github.com/jonathanlctt/ihs_srht}{GitHub}



\newpage



\section{Ridge regression}

\begin{itemize}
\item
\href{https://www.jmlr.org/papers/volume18/17-313/17-313.pdf}{Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging}
\item
\href{https://arxiv.org/pdf/1511.07263.pdf}{Input Sparsity Time Low-Rank Approximation via Ridge Leverage Score Sampling}
\item
\href{https://proceedings.mlr.press/v80/chowdhury18a/chowdhury18a.pdf}{An Iterative, Sketching-based Framework for Ridge Regression}: focuses on underdetermined case $d\gg n$
\end{itemize}

Ridge regression--does not need assumption that $\bfX$ is full rank?:
\begin{align*}
&\argmin_{\bfb\in\bbR^d} \frac{1}{2}\|\bfX\bfb-\bfy\|^2 + \frac{\lambda}{2}\|\bfb\|^2 \\
&= \argmin_{\bfb\in\bbR^d} \frac{1}{2}\left(\bfb^\T\bfX^\T\bfX\bfb - 2\bfb^\T\bfX^\T\bfy + \bfy^T\bfy\right) + \frac{\lambda}{2}\bfb^T\bfb \\
\end{align*}
Gradient:
\[
\nabla f(\bfb_t) = \bfX^\T\bfX\bfb_t - \bfX^\T\bfy + \lambda\bfb_t = \left(\bfX^\T\bfX+\lambda\bfI_d\right)\bfb_t - \bfX^\T\bfy
\]
Note: the following are equivalent solutions (useful for overdetermined, underdetermined case?)
\begin{align*}
\bfb^* &= \left(\bfX^\T\bfX+\lambda\bfI_d\right)^{-1}\bfX^\T\bfy \\
\bfb^* &= \bfX^\T\left(\bfX\bfX^\T+\lambda\bfI_n\right)^{-1}\bfy
\end{align*}
Hessian and partial Newton sketch (as considered by \citet{Chowdhury:2018}, \citet{Wang:2017}):
\begin{align*}
\bfH &= \bfX^\T\bfX+\lambda\bfI_d \\
\bfH_t &= \bfX^\T\bfS_t^\T\bfS_t\bfX + \lambda\bfI_d
\end{align*}
IHS with no momentum and partial sketch:
\begin{align*}
\bfb_{t+1} &= \bfb_t - \alpha_t\bfH_t^{-1}\nabla f(\bfb_t) \\
\bfb_{t+1} &= \bfb_t - \alpha_t\left(\bfX^\T\bfS_t^\T\bfS_t\bfX + \lambda\bfI_d\right)^{-1}\left(\left(\bfX^\T\bfX+\lambda\bfI_d\right)\bfb_t-\bfX^\T\bfy\right) \\
&= \bfb_t - \alpha_t\left(\bfX^\T\bfS_t^\T\bfS_t\bfX + \lambda\bfI_d\right)^{-1}\left(\left(\bfX^\T\bfX+\lambda\bfI_d\right)\bfb_t-\left(\bfX^\T\bfX+\lambda\bfI_d\right)\bfb^*\right) \\
&= \bfb_t - \alpha_t\left(\bfX^\T\bfS_t^\T\bfS_t\bfX + \lambda\bfI_d\right)^{-1}\left(\bfX^\T\bfX+\lambda\bfI_d\right)\left(\bfb_t-\bfb^*\right) \\
\bfX\bfb_{t+1} - \bfX\bfb^* &= \bfX\bfb_t - \alpha_t\bfX\left(\bfX^\T\bfS_t^\T\bfS_t\bfX + \lambda\bfI_d\right)^{-1}\left(\bfX^\T\bfX+\lambda\bfI_d\right)\left(\bfb_t-\bfb^*\right) - \bfX\bfb^* \\
&= \bfX\bfb_t - \alpha_t\left(\bfQ_t\bfb_t-\bfQ_t\bfb^*\right) - \bfX\bfb^*
\end{align*}
Let $\bfX = \bfU\Sigma\bfV^\T$ where $\bfU$, $\bfV$ orthogonal and $\Sigma$ diagonal. \todo: take compact SVD? might break if multiplying by $\bfX$
\begin{align*}
\bfX^\T\bfX+\lambda\bfI_d &= \bfV\Sigma^\T\Sigma\bfV^\T + \lambda \bfV\bfV^\T \\
&= \bfV\left(\Sigma^\T\Sigma + \lambda\bfI_d\right)\bfV^\T \\
\left(\bfX^\T\bfS_t^\T\bfS_t\bfX + \lambda\bfI_d\right)^{-1} &= \left(\bfV\Sigma^\T\bfU^\T\bfS_t^\T\bfS_t\bfU\Sigma\bfV^\T + \lambda\bfV\bfV^\T\right)^{-1} \\
&= \bfV\left(\Sigma^\T\bfU^\T\bfS_t^\T\bfS_t\bfU\Sigma + \lambda\bfI_d\right)^{-1}\bfV^\T
\end{align*}
Then
\begin{align*}
\bfb_{t+1} &= \bfb_t - \alpha_t\left(\bfX^\T\bfS_t^\T\bfS_t\bfX + \lambda\bfI_d\right)^{-1}\left(\bfX^\T\bfX+\lambda\bfI_d\right)\left(\bfb_t-\bfb^*\right) \\
&= \bfb_t - \alpha_t\bfV\left(\Sigma^\T\bfU^\T\bfS_t^\T\bfS_t\bfU\Sigma + \lambda\bfI_d\right)^{-1}\left(\Sigma^\T\Sigma + \lambda\bfI_d \right)\bfV^\T\left(\bfb_t-\bfb^*\right) \\
\bfV^\T(\bfb_{t+1}-\bfb^*) &= \bfV^\T\bfb_t - \alpha_t\left(\Sigma^\T\bfU^\T\bfS_t^\T\bfS_t\bfU\Sigma + \lambda\bfI_d\right)^{-1}\left(\Sigma^\T\Sigma + \lambda\bfI_d\right)\bfV^\T\left(\bfb_t-\bfb^*\right) - \bfV^\T\bfb^* \\
&= \left(\bfI_d - \alpha_t\left(\Sigma^\T\bfU^\T\bfS_t^\T\bfS_t\bfU\Sigma + \lambda\bfI_d\right)^{-1}\left(\Sigma^\T\Sigma + \lambda\bfI_d\right)\right)\bfV^\T\left(\bfb_t-\bfb^*\right) \\
\|\bfb_{t+1}-\bfb^*\|^2 &= \left(\bfb_t-\bfb^*\right)^\T\bfV\left(\bfI_d - \alpha_t\left(\Sigma^\T\bfU^\T\bfS_t^\T\bfS_t\bfU\Sigma + \lambda\bfI_d\right)^{-1}\left(\Sigma^\T\Sigma + \lambda\bfI_d\right)\right)^2\bfV^\T \left(\bfb_t-\bfb^*\right)
\end{align*}

\todo: take $\Delta_t=\bfV^\T(\bfb_t-\bfb^*)$? Interpretation in terms of solution error?
\begin{align*}
\|\Delta_{t+1}\|^2 &= \|\bfV^\T\left(\bfb_{t+1}-\bfb^*\right)\|^2 \\
&= \|\bfb_{t+1}-\bfb^*\|^2
\end{align*}
\begin{align*}
\|\Delta_{t+1}\|^2 &= \Delta_t^\T\left(\bfI_d - \alpha_t\left(\Sigma^\T\bfU^\T\bfS_t^\T\bfS_t\bfU\Sigma + \lambda\bfI_d\right)^{-1}\left(\Sigma^\T\Sigma + \lambda\bfI_d\right)\right)^2\Delta_t \\
\E\left[\|\Delta_{t+1}\|^2\right] &= \Delta_t^\T\E\left[\left(\bfI_d - \alpha_t\left(\Sigma^\T\bfU^\T\bfS_t^\T\bfS_t\bfU\Sigma + \lambda\bfI_d\right)^{-1}\left(\Sigma^\T\Sigma + \lambda\bfI_d\right)\right)^2\right]\Delta_t \\
&= \Delta_t^\T\left(\bfI_d - \alpha_t\E\left[\left(\Sigma^\T\bfU^\T\bfS_t^\T\bfS_t\bfU\Sigma + \lambda\bfI_d\right)^{-1}\right]\left(\Sigma^\T\Sigma + \lambda\bfI_d\right) \right.  \\
&\quad- \alpha_t\left(\Sigma^\T\Sigma + \lambda\bfI_d\right)\E\left[\left(\Sigma^\T\bfU^\T\bfS_t^\T\bfS_t\bfU\Sigma + \lambda\bfI_d\right)^{-1}\right] \\
&\quad + \left. \alpha_t^2\left(\Sigma^\T\Sigma + \lambda\bfI_d\right)\E\left[\left(\Sigma^\T\bfU^\T\bfS_t^\T\bfS_t\bfU\Sigma+ \lambda\bfI_d\right)^{-2}\right]\left(\Sigma^\T\Sigma + \lambda\bfI_d\right)\right)\Delta_t
\end{align*}
$\Sigma^\T\bfU^\T\bfS_t^\T\bfS_t\bfU\Sigma$ is positive semidefinite and $\lambda\bfI_d$ is positive definite. Then $\Sigma^\T\bfU^\T\bfS_t^\T\bfS_t\bfU\Sigma + \lambda\bfI_d$ is positive definite. Consider the eigendecomposition $\Sigma^\T\bfU^\T\bfS_t^\T\bfS_t\bfU\Sigma + \lambda\bfI_d = \bfQ\Lambda\bfQ^\T$ where $\Lambda$ is diagonal with positive entries $\lambda_1,\ldots,\lambda_d$ and $\bfQ\in\bbR^{d\times d}$ is orthogonal. \todo: can't make rotational invariance argument? Can method for Theorem~4.1 be used here?
\\

Assuming $\bfX$ full rank, if taking $\bfX=\bfU\Sigma\bfV^\T$ as thin SVD:
\begin{align*}
\bfb_{t+1} &= \bfb_t - \alpha_t\bfV\left(\Sigma\bfU^\T\bfS_t^\T\bfS_t\bfU\Sigma + \lambda\bfI_d\right)^{-1}\left(\Sigma^2 + \lambda\bfI_d \right)\bfV^\T\left(\bfb_t-\bfb^*\right) \\
&= \bfb_t - \alpha_t\bfV\Sigma^{-1}\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\Sigma + \lambda\Sigma^{-1} \right)\bfV^\T\left(\bfb_t-\bfb^*\right) \\
&= \bfb_t - \alpha_t\bfV\Sigma^{-1}\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2} \right)\Sigma\bfV^\T\left(\bfb_t-\bfb^*\right)
\end{align*}
If $\Delta_t=\Sigma\bfV^\T\left(\bfb_t-\bfb^*\right)$:
\begin{align*}
\Sigma\bfV^\T(\bfb_{t+1}-\bfb^*) &= \Sigma\bfV^\T\bfb_t - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2}\right)\Sigma\bfV^\T\left(\bfb_t-\bfb^*\right) - \Sigma\bfV^\T\bfb^* \\
&= \left(\bfI_d - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2}\right)\right)\Sigma\bfV^\T\left(\bfb_t-\bfb^*\right) \\
\|\Delta_{t+1}\|^2 &= \Delta_t^\T\left(\bfI_d - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2}\right)\right)^2\Delta_t \\
\E\left[\|\Delta_{t+1}\|^2\right] &= \Delta_t^\T\E\left[\left(\bfI_d - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2}\right)\right)^2\right]\Delta_t 
\end{align*}
If $\Delta_t=\bfU^\T\bfX\left(\bfb_t-\bfb^*\right)$:
\begin{align*}
\bfX(\bfb_{t+1}-\bfb^*) &= \bfX\bfb_t - \alpha_t\bfU\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2}\right)\Sigma\bfV^\T\left(\bfb_t-\bfb^*\right) - \bfX\bfb^* \\
&= \left(\bfI_d - \alpha_t\bfU\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2}\right)\bfU^T\right)\bfX\left(\bfb_t-\bfb^*\right) \\
\bfU^\T\bfX(\bfb_{t+1}-\bfb^*) &= \left(\bfU^\T - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2}\right)\bfU^T\right)\bfX\left(\bfb_t-\bfb^*\right) \\
&= \left(\bfI_d - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2}\right)\right)\bfU^\T\bfX\left(\bfb_t-\bfb^*\right) \\
\Delta_{t+1} &= \left(\bfI_d - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2}\right)\right)\Delta_t \\
\|\Delta_{t+1}\|^2 &= \Delta_t^\T\left(\bfI_d - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2}\right)\right)^2\Delta_t \\
\E\left[\|\Delta_{t+1}\|^2\right] &= \Delta_t^\T\E\left[\left(\bfI_d - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2}\right)\right)^2\right]\Delta_t  \\
&= \Delta_t^\T\left(\bfI_d - \alpha_t\E\left[\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\right]\left(\bfI_d + \lambda\Sigma^{-2}\right) \right. \\
&\quad - \alpha_t\left(\bfI_d + \lambda\Sigma^{-2}\right)\E\left[\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\right] \\
&\quad + \left. \alpha_t^2\left(\bfI_d + \lambda\Sigma^{-2}\right)\E\left[\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-2}\right]\left(\bfI_d + \lambda\Sigma^{-2}\right)\right)\Delta_t 
\end{align*}

\todo Assuming $\bfQ_t=\bfI_d - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2}\right)$ satisfies conditions (limiting spectral distribution as $n\rightarrow\infty$ and others?), under the additional assumption that $\Delta_0$ is random and $\E\left[\Delta_0\Delta_0^\T\right]=\frac{\bfI_d}{d}$, we have $\Delta_{t+1}=Q_t\Delta_t$ and so
\begin{align*}
\E\left[\|\Delta_{t+1}\|^2\right] &= \mathrm{trace}\left(\E\left[\Delta_0^\T\bfQ_0\ldots \bfQ_{t-1}\bfQ_{t-1}\ldots\bfQ_0\Delta_0\right]\right) \\
&= \mathrm{trace}\left(\E\left[\bfQ_0\ldots \bfQ_{t-1}\bfQ_{t-1}\ldots\bfQ_0\Delta_0\Delta_0^\T\right]\right) \\
&= \mathrm{trace}\left(\E\left[\bfQ_0\ldots \bfQ_{t-1}\bfQ_{t-1}\ldots\bfQ_0\right]\E\left[\Delta_0\Delta_0^\T\right]\right) \\
&= \frac{1}{d}\mathrm{trace}\left(\E\left[\bfQ_0\ldots \bfQ_{t-1}\bfQ_{t-1}\ldots\bfQ_0\right]\right)
\end{align*}
using the independence of $\Delta_0$ and $\bfQ_i$. Then
\begin{align*}
&& \E\left[\|\Delta_{t+1}\|^2\right] &= \frac{1}{d}\mathrm{trace}\left(\E\left[\bfQ_1\ldots \bfQ_{t-1}\bfQ_{t-1}\ldots\bfQ_0^2\right]\right) \\
&\Rightarrow & \lim_{n\rightarrow\infty}\E\left[\|\Delta_{t+1}\|^2\right] &= \lim_{n\rightarrow\infty}\frac{1}{d}\mathrm{trace}\left(\E\left[\bfQ_1\ldots \bfQ_{t-1}\bfQ_{t-1}\ldots\bfQ_0^2\right]\right) \\
&& &= \lim_{n\rightarrow\infty}\frac{1}{d}\mathrm{trace}\left(\E\left[\bfQ_0^2\right]\right)\lim_{n\rightarrow\infty}\frac{1}{d}\mathrm{trace}\left(\E\left[\bfQ_2\ldots \bfQ_{t-1}\bfQ_{t-1}\ldots\bfQ_1^2\right]\right) \\
&&&= \prod_{i=0}^{t-1}\lim_{n\rightarrow\infty}\frac{1}{d}\mathrm{trace}\left(\E\left[\bfQ_i^2\right]\right)
\end{align*}
using the fact that $\bfQ_0^2$ is asymptotically free from $\bfQ_{t-1}\ldots\bfQ_1$ and recursing. The expectation of $\bfQ_i$ is given by
\begin{align*}
\E\left[\bfQ_i^2\right] &= \bfI_d - \alpha_i\E\left[\left(\bfU^\T\bfS_i^\T\bfS_i\bfU + \lambda\Sigma^{-2}\right)^{-1}\right]\left(\bfI_d + \lambda\Sigma^{-2}\right) \\
&\quad - \alpha_i\left(\bfI_d + \lambda\Sigma^{-2}\right)\E\left[\left(\bfU^\T\bfS_i^\T\bfS_i\bfU + \lambda\Sigma^{-2}\right)^{-1}\right] \\
&\quad + \alpha_i^2\left(\bfI_d + \lambda\Sigma^{-2}\right)\E\left[\left(\bfU^\T\bfS_i^\T\bfS_i\bfU + \lambda\Sigma^{-2}\right)^{-2}\right]\left(\bfI_d + \lambda\Sigma^{-2}\right)
\end{align*}
and the normalized limiting trace is given by
\begin{align*}
\lim_{n\rightarrow\infty}\frac{1}{d}\mathrm{trace}\left(\E\left[\bfQ_i^2\right]\right) &= 1 - \frac{2\alpha_i}{d}\lim_{n\rightarrow\infty}\mathrm{trace}\left(\E\left[\left(\bfU^\T\bfS_i^\T\bfS_i\bfU + \lambda\Sigma^{-2}\right)^{-1}\right]\left(\bfI_d + \lambda\Sigma^{-2}\right)\right) \\
&\quad + \frac{\alpha_i^2}{d}\lim_{n\rightarrow\infty}\mathrm{trace}\left(\E\left[\left(\bfU^\T\bfS_i^\T\bfS_i\bfU + \lambda\Sigma^{-2}\right)^{-2}\right]\left(\bfI_d + \lambda\Sigma^{-2}\right)^2\right)
\end{align*}
\todo problem: cannot derive limiting spectral distribution of $\bfU^\T\bfS_i^\T\bfS_i\bfU + \lambda\Sigma^{-2}$ following approach of Lemma~A.1 as matrix is not orthogonal.


\subsection{Full Newton sketch}

\todo: instead of sketching $\bfX^\T\bfX$, can we sketch $\bfX^\T\bfX+\lambda\bfI_d$? For example, take Cholesky decomposition $\bfX^\T\bfX+\lambda\bfI_d = \bfA\bfA^\T$ and sketch $\bfA\bfS^\T\bfS\bfA^\T$. We only have to compute Cholesky once, but $\bfX^\T\bfX+\lambda\bfI_d$ still needs to be computed beforehand. Are there still computational benefits of sketching in this case?

Not useful.


\subsection{Scratch notes}

Newton's method with least squares:
\begin{align*}
\bfb_{t+1} &= \bfb_t - \alpha_t\bfH^{-1}\nabla f(\bfb_t) \\
\bfb_{t+1} &= \bfb_t - \alpha_t\left(\bfX^\T\bfX\right)^{-1}\left(\bfX^\T\bfX\bfb_t-\bfX^\T\bfy\right) \\
&= \bfb_t - \alpha_t\left(\bfb_t-\left(\bfX^\T\bfX\right)^{-1}\bfX^\T\bfy\right) \\
\bfX\bfb_{t+1} &= \bfX\bfb_t - \alpha_t\left(\bfX\bfb_t-\bfX\left(\bfX^\T\bfX\right)^{-1}\bfX^\T\bfy\right) \\
\bfX(\bfb_{t+1}-\bfb^*) &= \bfX\bfb_t - \alpha_t\left(\bfX\bfb_t-\bfX\left(\bfX^\T\bfX\right)^{-1}\bfX^\T\bfy\right) - \bfX\bfb^* \\
&= \bfX\bfb_t - \alpha_t\left(\bfX\bfb_t-\bfX\bfb^*\right) - \bfX\bfb^* \\
&= \left(\bfI_n - \alpha_t\bfI_n\right)\bfX(\bfb_t-\bfb^*)
\end{align*}
IHS: let $\bfX=\bfU\Sigma\bfV^\T$ where $\bfU$, $\bfV$ orthogonal and $\Sigma$ diagonal. Then
\begin{align*}
\bfX\left(\bfX^\T\bfS_t^\T\bfS_t\bfX\right)^{-1}\bfX^T &= \bfU\Sigma\bfV^\T\left(\bfV\Sigma\bfU^\T\bfS_t^\T\bfS_t\bfU\Sigma\bfV^\T\right)^{-1}\bfV\Sigma\bfU^\T \\
&= \bfU\left(\bfU^\T\bfS_t^\T\bfS_t\bfU\right)^{-1}\bfU^\T
\end{align*}
\begin{align*}
\bfb_{t+1} &= \bfb_t - \alpha_t\left(\bfX^\T\bfS_t^\T\bfS_t\bfX\right)^{-1}\left(\bfX^\T\bfX\bfb_t-\bfX^\T\bfy\right) \\
&= \bfb_t - \alpha_t\left(\bfX^\T\bfS_t^\T\bfS_t\bfX\right)^{-1}\bfX^\T\bfX(\bfb_t-\bfb^*) \\
\bfX\bfb_{t+1} - \bfX\bfb^* &= \bfX\bfb_t - \alpha_t\bfX\left(\bfX^\T\bfS_t^\T\bfS_t\bfX\right)^{-1}\bfX^\T\bfX(\bfb_t-\bfb^*) - \bfX\bfb^* \\
&= \bfX\bfb_t - \alpha_t\bfU\left(\bfU^\T\bfS_t^\T\bfS_t\bfU\right)^{-1}\bfU^\T\bfX(\bfb_t-\bfb^*) - \bfX\bfb^* \\
&= \left(\bfI_n - \alpha_t\bfU\left(\bfU^\T\bfS_t^\T\bfS_t\bfU\right)^{-1}\bfU^\T\right)\bfX(\bfb_t-\bfb^*)
\end{align*}
Let $\Delta_t = \bfU^\T\bfX(\bfb_t-\bfb^*)$. Then
\begin{align*}
\Delta_{t+1} &= \left(\bfU^\T - \alpha_t\bfU^\T\bfU\left(\bfU^\T\bfS_t^\T\bfS_t\bfU\right)^{-1}\bfU^\T\right)\bfX(\bfb_t-\bfb^*) \\
&= \left(\bfI_d - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU\right)^{-1}\right)\Delta_t \\
\|\Delta_{t+1}\|^2 &= \Delta_t^\T\left(\bfI_d - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU\right)^{-1}\right)\Delta_t
\end{align*}

Note: Hessian approximated before simplification; otherwise work does not follow
\begin{align*}
\bfX(\bfb_{t+1}-\bfb^*) &= \bfX\bfb_t - \alpha_t\left(\bfX\bfb_t-\bfX\left(\bfX^\T\bfS_t^\T\bfS_t\bfX\right)^{-1}\bfX^\T\bfy\right) - \bfX\bfb^* \\
&= \bfX\bfb_t - \alpha_t\left(\bfX\bfb_t-\bfU\left(\bfU^\T\bfS_t^\T\bfS_t\bfU\right)^{-1}\bfU^\T\bfy\right) - \bfX\bfb^*
\end{align*}



\newpage



\section{Weighted least squares}

\begin{itemize}
\item
\href{https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570596.pdf}{An Analysis of Sketched IRLS for Accelerated Sparse Residual Regression}: iterative method based on classical sketch
\end{itemize}
Setup:
\[
\argmin_{\bfb\in\bbR^d} \frac{1}{2}\|\bfW\bfX\bfb-\bfW\bfy\|^2
\]



\newpage



\section{Scratch notes}

\subsection{Random matrix theory}

\begin{itemize}

\item
Stieltjes transform: \\
\url{https://mathoverflow.net/questions/79109/intuitive-understanding-of-the-stieltjes-transform} \\
\url{https://terrytao.wordpress.com/2010/02/02/254a-notes-4-the-semi-circular-law/} \\
Berkeley lecture notes: \url{https://www.stat.berkeley.edu/~songmei/Teaching/STAT260_Spring2021/Lecture_notes/scribe_lecture17.pdf} \\
Distribution of sums of independent commutative r.v.'s can be quickly computed using characteristic function; analogous for distribution of sums of freely independent non-commutative r.v.'s is using the Stieltjes transform \\
Stieltjes transform of \esd of a random matrix is easy to compute \citep{Couillet:2011}

\item
$S$-transform: under suitable conditions, the $S$-transform of the \lsd of a matrix product $\bfA\bfB$ is the product of the $S$-transforms of the \lsd of $\bfA$ and the \lsd of $\bfB$ \citep{Couillet:2011}

\item
$\eta$-transform: purpose mainly to simplify long derivations involving Stieltjes transform \citep{Couillet:2011}

\item
Free probability (non-commutative random variables, e.g., random matrices): \\
\url{https://terrytao.wordpress.com/2010/02/10/245a-notes-5-free-probability/} \\
Light intro: \url{https://arxiv.org/pdf/1902.10763.pdf} \citep{Xia:2019}
\begin{itemize}
\item
Freeness between non-commutative random variables analogous to independence in commutative random variables
\item
Semicircular distribution of free CLT analogous to Gaussian distribution of classical CLT; the asymptotic distributions of eigenvalues of Hermition Gaussian random matrices (Wigner's semi-circle law); sub-Gaussian distribution \\
Example: \url{https://mathworld.wolfram.com/WignersSemicircleLaw.html}
\item
Cauchy transform = Stieltjes transform (or closely related)? Theorem~3 says that Cauchy transforms and probability measures on $\bbR$ are in one-to-one correspondence
\item
One method to construct Haar unitary matrices: take $n\times n$ random matrix with \iid Gaussian random variables and apply the Gram-Schmidt procedure
\end{itemize}

\item
Marchenko-Pastur proof given in \citep{Couillet:2011}

\item
\href{https://arxiv.org/pdf/1302.5688.pdf}{Asymptotic liberating sequences}

\end{itemize}


\subsection{Newton's method}

Problem: for $f$ twice-differentiable, minimize
\[
\min_{x\in\bbR} f(x) \;.
\]
Second-order Taylor expansion around iterate $x_t$:
\[
f(x_t+a) \approx f(x_t) + f'(x_t)a + \frac{1}{2}f''(x_t)a^2 \;.
\]
If second derivative positive, minimum at derivative zero and so
\[
f'(x_t) + f''(x_t)a = 0 \qquad \Rightarrow\qquad a = -\frac{f'(x_t)}{f''(x_t)} \;.
\]
Newton update:
\[
x_{t+1} = x_t +a = x_t - \frac{f'(x_t)}{f''(x_t)} \;.
\]


\subsection{Underdetermined least squares}

Is there anything interesting about IHS in the underdetermined least squares case?
\begin{itemize}

\item
For example, gradient descent in underdetermined least squares converges to the minimum norm solution (assuming solution initialized in orthogonal complement of null space of data matrix, i.e., row space). \\
\href{https://arxiv.org/pdf/1705.09280.pdf}{Implicit Regularization in Matrix Factorization}; \href{https://math.stackexchange.com/questions/3451272/does-gradient-descent-converge-to-a-minimum-norm-solution-in-least-squares-probl}{SE}

\item
If $n\ll d$, does it make more sense to reduce the feature dimension, i.e., $\tilde{A}=AS$ for $A$ $n times d$, $S$ $d\times m$, $m\ll d$? For fixed sketches? Otherwise are there any computational savings from sketching? Note $A^TA$ not invertible in this case but $AA^T$ is. \\
\href{https://proceedings.neurips.cc/paper/2019/file/51425b752a0b402ed3effc83fc4bbb74-Paper.pdf}{High-Dimensional Optimization in Adaptive Random Subspaces}: uses right sketching matrix, works with $AA^T$. \\
\href{https://arxiv.org/pdf/2006.05874.pdf}{Effective Dimension Adaptive Sketching Methods for Faster Regularized Least-Squares Optimization}: dual of underdetermined is overdetermined problem?

\item
Ridge regression allows for an unique solution. \\
\href{https://proceedings.mlr.press/v80/chowdhury18a/chowdhury18a.pdf}{An Iterative, Sketching-based Framework for Ridge Regression}

\end{itemize}


\subsection{Quantile regression}

Are there problems for which IHS could perform better than conventional methods?
\begin{itemize}

\item
Models conditional quantiles. Makes no assumptions about distribution of response nor homoscedasticity. Is equivariant to transformation. Need more data than linear regression and is computationally intensive. \\
\href{https://towardsdatascience.com/quantile-regression-ff2343c4a03}{What is quantile regression} \\
\href{https://support.sas.com/resources/papers/proceedings17/SAS0525-2017.pdf}{Five Things You Should Know about Quantile Regression} \\
\href{http://www.econ.uiuc.edu/~roger/research/rq/rq.pdf}{Quantile Regression} \\
\href{http://www.econ.uiuc.edu/~roger/research/nlrq/text/out.pdf}{An Interior Point Algorithm for Nonlinear Quantile Regression} \\
\href{https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152027}{An Improved Interior Point Algorithm for Quantile Regression} \\
\href{https://www.jstor.org/stable/25053439}{Computational Issues for Quantile Regression} \\
\href{https://mathweb.ucsd.edu/~xip024/Papers/sqr_main.pdf}{Smoothed Quantile Regression with Large-Scale Inference} \\
\href{https://www.taylorfrancis.com/chapters/edit/10.1201/9781315120256-5/computational-methods-quantile-regression-roger-koenker}{Computational Methods for Quantile Regression} \\
\href{https://proceedings.mlr.press/v28/yang13f.pdf}{Quantile Regression for Large-scale Applications}: pre-IHS stochastic algorithm? \\

\href{https://projecteuclid.org/journals/statistical-science/volume-12/issue-4/The-Gaussian-hare-and-the-Laplacian-tortoise--computability-of/10.1214/ss/1030037960.full}{The Gaussian hare and the Laplacian tortoise: computability of squared-error versus absolute-error estimators}: implemented in R; interior point method works by maximizing (differentiable) dual of primal function (minimizing check loss) and translating constraints into a barrier function. Under conditions (?), overall complexity is $O_p(n^{1+a}p^3\log n)$ given a $n\times p$ constraint matrix and $a<\frac{1}{2}$. \\
\href{https://epubs.siam.org/doi/book/10.1137/1.9781611971453}{Primal-Dual Interior-Point Methods} \\
\href{https://epubs.siam.org/doi/abs/10.1137/0802028?journalCode=sjope8}{On the Implementation of a Primal-Dual Interior Point Method}

\href{https://stanford.edu/~pilanci/papers/PilWai17.pdf}{Newton Sketch: A Linear-time Optimization Algorithm with Linear-Quadratic Convergence}

\end{itemize}


\subsection{Gradient flow}

Proximal algorithms?
\begin{itemize}
\item
\href{https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf}{Proximal algorithms}
\end{itemize}


\subsection{Related literature}

\begin{itemize}

\item
Extensions of IHS: \\
\href{https://arxiv.org/pdf/2002.06540.pdf}{Distributed Averaging Methods for Randomized Second Order Optimization} \\
\href{https://ieeexplore.ieee.org/document/8682720}{Iterative Hessian Sketch with Momentum} \todo: how does this work differ or not contradict the current work? \\
\href{https://proceedings.mlr.press/v119/lacotte20a/lacotte20a.pdf}{Optimal Randomized First-Order Methods for Least-Squares Problems}: extension of current work with non-refreshed sketches \\
\href{https://arxiv.org/pdf/2105.07291.pdf}{Adaptive Newton Sketch: Linear-time Optimization with Quadratic Convergence and Effective Hessian Dimensionality} \\
\href{https://arxiv.org/pdf/2107.07480.pdf}{Newton-LESS: Sparsification without Trade-offs for the Sketched Newton Update}: sparse IHS? \\
\href{https://proceedings.neurips.cc/paper/2019/file/1f36c15d6a3d18d52e8d493bc8187cb9-Paper.pdf}{Asymptotics for Sketching in Least Squares}: referenced paper in current work; compares one-step sketch matrices

\item
Applications: \\
\href{https://projecteuclid.org/journals/annals-of-statistics/volume-45/issue-3/Randomized-sketches-for-kernels-Fast-and-optimal-nonparametric-regression/10.1214/16-AOS1472.full}{Randomized sketches for kernels: Fast and optimal nonparametric regression}

\item
Possibly helpful: \\
\href{https://arxiv.org/pdf/2201.00450.pdf}{On randomized sketching algorithms and the Tracy-Widom law} \\
\href{https://arxiv.org/pdf/1609.00048.pdf}{Practical sketching algorithms for low-rank matrix approximation} \\
\href{https://www.jmlr.org/papers/volume17/15-440/15-440.pdf}{A Statistical Perspective on Randomized Sketching for Ordinary Least-Squares}: complementary to 2016 IHS paper \\
\href{https://dl.acm.org/doi/10.1145/3034786.3056119}{Efficient Matrix Sketching over Distributed Data}: PCA? \\
\href{https://epubs.siam.org/doi/pdf/10.1137/15M1025487}{Randomized Iterative Methods for Linear Systems}: classical sketch \\
\href{https://arxiv.org/pdf/1507.02268.pdf}{Optimal approximate matrix product in terms of stable rank}: error bounds on matrix approximations

\item
Related to expectation of solution? \\
\href{https://arxiv.org/pdf/1906.11809v1.pdf}{High-Dimensional Optimization in Adaptive Random Subspaces} \\
\href{https://projecteuclid.org/journals/annals-of-statistics/volume-45/issue-3/Randomized-sketches-for-kernels-Fast-and-optimal-nonparametric-regression/10.1214/16-AOS1472.full}{Randomized sketches for kernels: Fast and optimal nonparametric regression}

\item
Underdetermined? \\
\href{https://arxiv.org/pdf/2006.05874.pdf}{Effective Dimension Adaptive Sketching Methods for Faster Regularized Least-Squares Optimization}: refreshed embeddings does not improve on fixed embedding?
\begin{itemize}
\item
\href{https://arxiv.org/pdf/1911.02675.pdf}{Faster Least Squares Optimization}
\item
\href{https://proceedings.mlr.press/v119/lacotte20a/lacotte20a.pdf}{Optimal Randomized First-Order Methods for Least-Squares Problems}: first-order method with fixed sketches that provide better guarantees.
\end{itemize}

\end{itemize}



\newpage

\bibliographystyle{plainnat}
\bibliography{../references/qp}

\end{document}
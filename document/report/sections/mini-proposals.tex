% !TEX root = ../main.tex

% Mini-proposals section

\section{Mini-proposals}

% each mini-proposal gets its own subsection
\subsection*{A sketched interior point algorithm for quantile regression} % enter your proposal title

Quantile regression~\citep{Koenker:1978} offers several advantages over linear regression, such as being able to model different quantiles (as opposed to only a mean), being free from assumptions regarding the parametric form of the response and homoscedasticity, and being transformation equivariant in its response~\citep{Rodriguez:2017}. Given a data matrix $\bfX\in\bbR^{n\times d}$, responses $\bfy\in\bbR^{n}$ and a quantile $\tau\in(0,1)$, quantile regression fits a linear model on the conditional quantile with the estimated parameters being the solution to the optimization problem
\[
\min_{\bfb\in\bbR^d} \; \sum_{i=1}^n(y_i-\bfx_i^\T\bfb)\left(\tau - \mathbbm{1}[y_i-\bfx_i^\top\bfb < 0]\right) \;.
\]
The objective is non-differentiable as-is but can be optimized as a linear program. For large data problems, the interior point method transforms the dual program into a constrained optimization problem using log barriers~\citep{Portnoy:1997}, i.e.,
\begin{align*}
\argmax_\bfa&\quad \bfy^\T\bfa & &\Longrightarrow\qquad & 
\argmax_\bfa&\; \bfy^\T\bfa+\mu\sum_{i=1}^n\log a_i \\
\text{s.t.}&\;\begin{cases}
\bfX^\T\bfa=(1-\tau)\bfX^\T\onevec_n \\ \bfa\in[0,1]^n 
\end{cases} &&& \text{s.t.}&\; \bfX^\T\bfa=(1-\tau)\bfX^\T\onevec_n \;.
\end{align*}
The interior point method solves the dual program by taking a sequence of Newton steps with $\mu\rightarrow0$. The solution to the Newton step in each iteration satisfies the equation
\[
\bfX^\T\bfW_t\bfX\bfb_t = \bfX^\T\bfW_t\left(\bfy+\mu\bfA^{-1}\onevec_n\right) \;.
\]
where $\bfA$ is a $n\times n$ diagonal matrix and $\bfW_t$ is a $n\times n$ diagonal matrix with positive diagonal entries that update every iteration. Computing $\bfX^\T\bfW_t\bfX$ is the main computational bottleneck that leads to each iteration having a cost of $O(nd^2)$~\citep{Chen:2005}.
\\

In this proposed project, we consider the case where $d\ll n$ and propose a stochastic interior point algorithm that uses sketching matrices to reduce the cost of the iterative updates. Drawing on methods from the sketching literature~\cite{Pilanci:2017}, the idea is to incorporate partial sketching into the original algorithm where instead of computing $\bfX^\T\bfW_t\bfX$, we compute $\bfX^\T\bfW_t^\frac{1}{2}\bfS_t^\T\bfS_t\bfW_t^\frac{1}{2}\bfX$.
The matrix $\bfS_t\in\bbR^{m\times n}$, $m\ll n$, is a dimension-reducing random matrix regenerated every iteration. For example, the subsampled randomized Hadamard transform allows the sketch $\bfS_t\bfW_t^\frac{1}{2}\bfX$ to be formed at a cost of $O(nd\log m)$~\citep{Lacotte:2020}, and so the matrix product above can be computed at a cost of $O(md^2)$. While the sketched solution will only be an approximation of the original, recent work on the convergence of sketching in other optimization problems show promising theoretical and empirical results~\citep[e.g.,][]{Pilanci:2017,Derezinski:2021,Lacotte:2021}. We note that \citet{Yang:2013} had previously proposed a stochastic algorithm for quantile regression. However, their method differs from ours in that they construct a random preconditioning matrix before using standard methods to solve the optimization problem on the conditioned data matrix.
\\

The main contributions of this project would be as follows:
\begin{enumerate}
\item
A sketched interior point algorithm for optimizing quantile regression problems that is expected to be faster than standard methods currently used in practice.
\item
A theoretical analysis of the proposed sketched interior point algorithm that provides convergence guarantees.
\item
An empirical comparison of the proposed sketching algorithm and other existing methods for quantile regression, such as the standard interior point method~\citep{Portnoy:1997} (implemented in R), the stochastic method by \citet{Yang:2013}, a more modern iteration of the interior point method~\citep{Zhao:2020}, and a modern quantile regression algorithm based on smoothing~\citep{He:2021} (also implemented in R).
\item
An implemention of the sketch interior point algorithm, e.g., in R, if found to have practical advantages over the existing algorithms.
\end{enumerate}

The main challenge in this project would be the theoretical analysis of the sketched interior point algorithm. The most feasible analysis approach would likely be following that of \citet{Pilanci:2017} for interior point methods and partial sketches, which would provide a worst-case convergence guarantee for the number of iterations needed to obtain a solution within a desired tolerance. Understanding the effects of different sketching matrices may also be of interest, but an analysis approach remnant to that of \citet{Lacotte:2020} would likely be necessary. However, adapting their asymptotic approach to that of quantile regression is not straightforward and would likely be more suited for a follow-up project.
\\

Following the completion of this project, there are multiple directions of future work that may be of interest:
\begin{enumerate}
\item
The proposed sketched interior point algorithm would not be useful for the $n\ll d$ case. A sketch-based method would likely still be possible but would need to use sketches differently, e.g., directly sketching the data matrix as \citet{Pham:2015} did for LASSO, or sketching both the data matrix and the observations as in classical least-squares sketch (although this has been shown to lead to suboptimal performance~\citep{Pilanci:2016}).
\item
Applications of quantile regression or interior point algorithms in general, e.g., composite quantile regression~\citep{Zou:2008} for high-dimensional regression, applications of quadratic progamming, may benefit from the faster algorithm.
\item
Sketch-based smoothing algorithms for quantile regression. These algorithms approximate the original optimization problem by a differentiable one and therefore sketching should directly follow from the work of \citet{Pilanci:2017}. Given the more standard setup, it would likely be easier to analyze these algorithms than the interior point algorithms.
\end{enumerate}

\iffalse
\newpage


% each mini-proposal gets its own subsection
\subsection{Proposal 2: MY OTHER PROPOSAL TITLE} % enter your proposal title

% ...
\fi
% !TEX root = ../main.tex

% Project report section

\section{Project report}

\subsection{Introduction}

Ridge regression is a special case of regularized least squares where the penalty function is chosen to be the $\ell_2$-norm of the model parameters. Given data matrix $\bfX\in\bbR^{n\times d}$, observations $\bfy\in\bbR^n$ and a regularization parameter $\lambda>0$, ridge regression obtains estimates of the parameters as the solution to the problem
\[
\bfb^* = \argmin_{\bfb\in\bbR^d} \frac{1}{2}\|\bfX\bfb-\bfy\|^2 + \frac{\lambda}{2}\|\bfb\|^2 \;.
\]
While ridge regression can be motivated as a method to reduce overfitting in an ordinary least squares (OLS) model, it also has its computational and analytical benefits over OLS. When $\bfX$ does not have full column rank (e.g., when $n< d$), then $\bfX^\T\bfX$ is singular and the OLS solution is non-unique. When $\bfX$ is full rank but ill-conditioned, then small changes in $\bfX$ lead to large changes in $(\bfX^\T\bfX)^{-1}$ and consequently the OLS solution. Ridge regression addresses both of these issues by minimizing the variance and mean squared error at the cost of introducing a small bias~\citep{Chowdhury:2018}. The ridge regression solution is unique and is given by
\[
\bfb^* = \left(\bfX^\T\bfX + \lambda\bfI_d\right)^{-1}\bfX^\T\bfy \;.
\]
In this report, we consider a sketched version \todo

While ridge regression can be considered a simple and small extension to OLS, we show that the analysis approach of \citet{Lacotte:2020} for IHS (\todo: define IHS) does not easily extend to a similar Newton sketch method for ridge regression.

\subsection{Background}

\subsubsection{Sketching and Newton sketch}

\subsubsection{Optimal convergence of iterative Hessian sketch}

\subsubsection{Related work}

\subsection{Notation}

\subsection{Newton sketch for ridge regression}

\todo
Gradient:
\[
\nabla f(\bfb_t) = \left(\bfX^\T\bfX+\lambda\bfI_d\right)\bfb_t - \bfX^\T\bfy
\]

Hessian and partial Newton sketch (as considered by \citet{Chowdhury:2018}, \citet{Wang:2017}):
\begin{align*}
\bfH &= \bfX^\T\bfX+\lambda\bfI_d \\
\bfH_t &= \bfX^\T\bfS_t^\T\bfS_t\bfX + \lambda\bfI_d
\end{align*}

No momentum
\begin{align*}
\bfb_{t+1} &= \bfb_t - \alpha_t\bfH_t^{-1}\nabla f(\bfb_t) \\
&= \bfb_t - \alpha_t\left(\bfX^\T\bfS_t^\T\bfS_t\bfX + \lambda\bfI_d\right)^{-1}\left(\left(\bfX^\T\bfX+\lambda\bfI_d\right)\bfb_t-\bfX^\T\bfy\right)
\end{align*}

\subsection{Analysis, assumptions and challenges}

\todo

Assumption~1: $\bfX$ has full column rank.

Define $\Delta_t=\bfU^\T\bfX\left(\bfb_t-\bfb^*\right)$.

Assumption~2: $\Delta_0$ is random and $\E\left[\Delta_0\Delta_0^\T\right]=\frac{\bfI_d}{d}$


Using the fact that the ridge regression solution satisfies the equation
\[
(\bfX^\T\bfX + \lambda\bfI_d)\bfb^* = \bfX^\T\bfY \;,
\]
the \todo iteration can be rewritten as
\begin{align*}
\bfb_{t+1} &= \bfb_t - \alpha_t\left(\bfX^\T\bfS_t^\T\bfS_t\bfX + \lambda\bfI_d\right)^{-1}\left(\left(\bfX^\T\bfX+\lambda\bfI_d\right)\bfb_t-\left(\bfX^\T\bfX+\lambda\bfI_d\right)\bfb^*\right) \\
&= \bfb_t - \alpha_t\left(\bfX^\T\bfS_t^\T\bfS_t\bfX + \lambda\bfI_d\right)^{-1}\left(\bfX^\T\bfX+\lambda\bfI_d\right)\left(\bfb_t-\bfb^*\right) \;.
\end{align*}
Let $\bfX = \bfU\Sigma\bfV^\T$ be the thin singular value decomposition of $\bfX$. By Assumption~1, $\bfU$ is a $n\times d$ semi-orthogonal matrix, $\bfV$ is a $d\times d$ orthogonal matrix, and $\Sigma$ is a $d\times d$ diagonal matrix with the non-zero singular values of $\bfX$ on the diagonal. Then we can write
\begin{align*}
\bfX^\T\bfX+\lambda\bfI_d &= \bfV\Sigma^2\bfV^\T + \lambda \bfV\Sigma\Sigma^{-2}\Sigma\bfV^\T \\
&= \bfV\Sigma\left(\bfI_d + \lambda\Sigma^{-2}\right)\Sigma\bfV^\T \\
\left(\bfX^\T\bfS_t^\T\bfS_t\bfX + \lambda\bfI_d\right)^{-1} &= \left(\bfV\Sigma^\T\bfU^\T\bfS_t^\T\bfS_t\bfU\Sigma\bfV^\T + \lambda\bfV\Sigma\Sigma^{-2}\Sigma\bfV^\T\right)^{-1} \\
&= \bfV\Sigma^{-1}\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\Sigma^{-1}\bfV^\T
\end{align*}
The \todo iteration then becomes
\[
\bfb_{t+1} = \bfb_t - \alpha_t\bfV\Sigma^{-1}\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2} \right)\Sigma\bfV^\T\left(\bfb_t-\bfb^*\right) \;.
\]
Multiplying both sides by $\bfU^\T\bfX$ then gives
\begin{align*}
\bfU^\T\bfX\bfb_{t+1} &= \bfU^\T\bfX\bfb_t - \alpha_t\bfU^\T\bfX\bfV\Sigma^{-1}\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2} \right)\Sigma\bfV^\T\left(\bfb_t-\bfb^*\right) \\
&= \bfU^\T\bfX\bfb_t - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2} \right)\Sigma\bfV^\T\left(\bfb_t-\bfb^*\right)
\end{align*}
and subtracting both sides by $\bfU^\T\bfX\bfb^*$ gives
\begin{align*}
\bfU^\T\bfX(\bfb_{t+1}-\bfb^*) &= \bfU^\T\bfX(\bfb_t-\bfb^*) - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2} \right)\Sigma\bfV^\T\left(\bfb_t-\bfb^*\right) \\
&= \bfU^\T\bfX(\bfb_t-\bfb^*) - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2} \right)\bfU^\T\bfX\left(\bfb_t-\bfb^*\right) \\
&= \left(\bfI_d - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2}\right)\right)\bfU^\T\bfX\left(\bfb_t-\bfb^*\right) \;.
\end{align*}
Therefore, by definition,
\[
\Delta_{t+1} = \left(\bfI_d - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2}\right)\right)\Delta_t
\]
and
\[
\|\Delta_{t+1}\|^2 = \Delta_t^\T\left(\bfI_d - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2}\right)\right)^2\Delta_t \;.
\]
Taking the expectation with respect to $\bfS_t$, we get
\[
\E\left[\|\Delta_{t+1}\|^2\right] = \Delta_t^\T\E\left[\left(\bfI_d - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2}\right)\right)^2\right]\Delta_t \;.
\]
\todo Theorem~4.1 avoids Assumption~2 by exploiting the rotational invariance of $\bfU^\T\bfS_t^\T\bfS_t\bfU$ \todo.

Take $\bfQ_t=\bfI_d - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2}\right)$. \todo satisfies conditions (limiting spectral distribution as $n\rightarrow\infty$ and others?). Then we have $\Delta_{t+1}=\bfQ_t\Delta_t$ and so
\begin{align*}
\E\left[\|\Delta_{t+1}\|^2\right] &= \mathrm{trace}\left(\E\left[\Delta_0^\T\bfQ_0\ldots \bfQ_{t-1}\bfQ_{t-1}\ldots\bfQ_0\Delta_0\right]\right) \\
&= \mathrm{trace}\left(\E\left[\bfQ_0\ldots \bfQ_{t-1}\bfQ_{t-1}\ldots\bfQ_0\Delta_0\Delta_0^\T\right]\right) \\
&= \mathrm{trace}\left(\E\left[\bfQ_1\ldots \bfQ_{t-1}\bfQ_{t-1}\ldots\bfQ_1\bfQ_0^2\right]\E\left[\Delta_0\Delta_0^\T\right]\right) \\
&= \frac{1}{d}\mathrm{trace}\left(\E\left[\bfQ_1\ldots \bfQ_{t-1}\bfQ_{t-1}\ldots\bfQ_1\bfQ_0^2\right]\right)
\end{align*}
using the independence of $\Delta_0$ and $\bfQ_i$ and using Assumption~2. Then taking the limit in $n$ and recursively applying the fact that $\bfQ_0^2$ is asymptotically free from $\bfQ_{t-1}\ldots\bfQ_1$, we get
\begin{align*}
\lim_{n\rightarrow\infty}\E\left[\|\Delta_{t+1}\|^2\right] &= \lim_{n\rightarrow\infty}\frac{1}{d}\mathrm{trace}\left(\E\left[\bfQ_1\ldots \bfQ_{t-1}\bfQ_{t-1}\ldots\bfQ_1\bfQ_0^2\right]\right) \\
&= \lim_{n\rightarrow\infty}\frac{1}{d}\mathrm{trace}\left(\E\left[\bfQ_0^2\right]\right)\lim_{n\rightarrow\infty}\frac{1}{d}\mathrm{trace}\left(\E\left[\bfQ_2\ldots \bfQ_{t-1}\bfQ_{t-1}\ldots\bfQ_2\bfQ_1^2\right]\right) \\
&= \prod_{i=0}^{t-1}\lim_{n\rightarrow\infty}\frac{1}{d}\mathrm{trace}\left(\E\left[\bfQ_i^2\right]\right)
\end{align*}
\todo The expectation of $\bfQ_i$ is given by
\begin{align*}
\E\left[\bfQ_i^2\right] &= \bfI_d - \alpha_i\E\left[\left(\bfU^\T\bfS_i^\T\bfS_i\bfU + \lambda\Sigma^{-2}\right)^{-1}\right]\left(\bfI_d + \lambda\Sigma^{-2}\right) \\
&\quad - \alpha_i\left(\bfI_d + \lambda\Sigma^{-2}\right)\E\left[\left(\bfU^\T\bfS_i^\T\bfS_i\bfU + \lambda\Sigma^{-2}\right)^{-1}\right] \\
&\quad + \alpha_i^2\left(\bfI_d + \lambda\Sigma^{-2}\right)\E\left[\left(\bfU^\T\bfS_i^\T\bfS_i\bfU + \lambda\Sigma^{-2}\right)^{-2}\right]\left(\bfI_d + \lambda\Sigma^{-2}\right)
\end{align*}
and the normalized limiting trace is given by
\begin{align*}
\lim_{n\rightarrow\infty}\frac{1}{d}\mathrm{trace}\left(\E\left[\bfQ_i^2\right]\right) &= 1 - \frac{2\alpha_i}{d}\lim_{n\rightarrow\infty}\mathrm{trace}\left(\E\left[\left(\bfU^\T\bfS_i^\T\bfS_i\bfU + \lambda\Sigma^{-2}\right)^{-1}\right]\left(\bfI_d + \lambda\Sigma^{-2}\right)\right) \\
&\quad + \frac{\alpha_i^2}{d}\lim_{n\rightarrow\infty}\mathrm{trace}\left(\E\left[\left(\bfU^\T\bfS_i^\T\bfS_i\bfU + \lambda\Sigma^{-2}\right)^{-2}\right]\left(\bfI_d + \lambda\Sigma^{-2}\right)^2\right)
\end{align*}
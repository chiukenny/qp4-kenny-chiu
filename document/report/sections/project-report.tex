% !TEX root = ../main.tex

% Project report section

\section{Project report}

\subsection{Introduction}

Ridge regression is a special case of regularized least squares where the penalty function is chosen to be the $\ell_2$-norm of the model parameters. Given data matrix $\bfX\in\bbR^{n\times d}$, observations $\bfy\in\bbR^n$ and a regularization parameter $\lambda>0$, ridge regression obtains estimates of the parameters as the solution to the optimization problem
\[
\bfb^* = \argmin_{\bfb\in\bbR^d} \frac{1}{2}\|\bfX\bfb-\bfy\|_2^2 + \frac{\lambda}{2}\|\bfb\|_2^2 \;.
\]
While ridge regression can be motivated as a method for reducing overfitting in ordinary least squares (OLS), it also has its computational and analytical benefits over OLS. When $\bfX$ does not have full column rank (e.g., when $n< d$), then $\bfX^\T\bfX$ is singular and the OLS solution is non-unique. When $\bfX$ is full rank but ill-conditioned, then small changes in $\bfX$ lead to large changes in $(\bfX^\T\bfX)^{-1}$ and consequently in the OLS solution. Ridge regression addresses both of these issues by minimizing the variance and mean squared error at the cost of introducing a small bias~\citep{Chowdhury:2018}. The ridge regression solution is unique and is given by
\[
\bfb^* = \left(\bfX^\T\bfX + \lambda\bfI_d\right)^{-1}\bfX^\T\bfy \;.
\]
In this report, we analyze the theoretical properties of a partial Newton sketch algorithm~\citep{Pilanci:2017} as an iterative solver for the ridge regression problem. In particular, we attempt to derive an optimal convergence rate and an optimal step size following the approach of \citet{Lacotte:2020} for iterative Hessian sketch with OLS using asymptotic results from random matrix theory and free probability. We show that while ridge regression can be considered a simple extension to OLS, extending the analysis approach of \citet{Lacotte:2020} to partial Newton sketch is not trivial. \todo
\\

This report is organized as follows: Section~\ref{sec:background} provides background about sketching and describes the OLS results by \citet{Lacotte:2020} that we aim to extend to ridge regression; Section~\ref{sec:literature} highlights relevant work in the literature; Section~\ref{sec:theory} discusses our attempts to analyze Newton sketch for ridge regression and the key differences from OLS that makes the problem challenging; Section~\ref{sec:empirical} describes \todo; and Section~\ref{sec:conclusion} summarizes our findings and concludes this report.

\subsection{Background} \label{sec:background}

This section briefly describes the Newton sketch algorithm and the probability subfields of random matrix theory and free probability that are referred to in this report.

\subsubsection{Newton's method and Newton sketch} \label{sec:newton}

Given a convex, twice-differentiable function $f:\bbR^d\rightarrow\bbR$, Newton's method is an efficient iterative method for finding the minimizing solution. The iterative updates are of the form
\[
\bfx_{t+1} = \bfx_t - \left(\nabla^2f(\bfx_t)\right)^{-1}\nabla f(\bfx_t)
\]
where $\nabla f(\bfx_t)$ and $\nabla^2f(\bfx_t)$ are the gradient and Hessian of $f$ evaluated at $\bfx_t$, respectively. Depending on the problem, computing the Hessian $\nabla^2f(\bfx_t)$ may be a computational bottleneck. The general Newton sketch algorithm~\citep{Pilanci:2017} avoids computing $\nabla^2f(\bfx_t)$ exactly and instead approximates it with
\[
\nabla^2f(\bfx_t) \approx \nabla^2f(\bfx_t)^\frac{1}{2}\bfS_t^\T\bfS_t\nabla^2f(\bfx_t)^\frac{1}{2}
\]
where $\bfS_t$ is a random, rectangular sketching matrix introduced for dimension reduction. In this project, we only consider refreshed sketches where $\bfS_0,\ldots,\bfS_t$ are \iid realizations (as opposed to a single, fixed realization). \todo: We also do not focus on the specific type of sketching matrix used as our findings hold generally across sketch types (e.g., \iid Gaussian or orthogonal sketches).
\\

For functions with an additive decomposition of the form $f=f_0+g$, it may be sufficient for computational improvements to only do a partial Newton sketch~\citep{Pilanci:2017} where the Hessian is approximated by
\[
\nabla^2f(\bfx_t) \approx \nabla^2f_0(\bfx_t)^\frac{1}{2}\bfS_t^\T\bfS_t\nabla^2f_0(\bfx_t)^\frac{1}{2} + \nabla^2g(\bfx_t) \;.
\]
In particular, the ridge regression loss function has this form where it is analytically more convenient to consider partial Newton sketch updates.

\subsubsection{Random matrix theory and free probability}

Free probability theory was initially developed by Voiculescu in the 1980's ~\citep{Anderson:2009} and is concerned with the study of non-commutative random variables. More recently, its connections to random matrix theory were established as a means of studying the limiting spectral distribution of random matrices. In free probability, the notion of freeness is analogous to independence in classical probability theory. A family of random $n\times n$ matrices $\{\bfX_n^{(1)},\ldots,\bfX_n^{(I)}\}$ is said to be asymptotically free~\citep{Couillet:2011_free} if
\begin{enumerate}
\item
$\bfX_n^{(i)}$ has a limiting spectral distribution for all $i\in\{1,\ldots,I\}$, and
\item
for all $\{i_1,\ldots,i_J\}$ with $i_j\in\{1,\ldots,I\}$, $j\in\{1,\ldots,J\}$ and $i_1\neq i_2,\ldots,i_{J-1}\neq i_J$, and for all polynomials $P_1,\ldots,P_J$ such that
\[
\lim_{n\rightarrow\infty} \frac{1}{n}\E\left[\mathrm{trace}\left(P_j\left(\bfX_n^{(i_j)}\right)\right)\right] = 0
\]
for all $j\in\{1,\ldots,J\}$, we have
\[
\lim_{n\rightarrow\infty} \frac{1}{n}\E\left[\mathrm{trace}\left(\prod_{j=1}^JP_j\left(\bfX_n^{(i_j)}\right)\right)\right] = 0 \;.
\]
\end{enumerate}
The analysis approach of \citet{Lacotte:2020} uses freeness to calculate the expected normalized trace of matrix products as for asymptotically free random matrices $\bfX_i$ and $\bfX_j$, it holds that as $n\rightarrow\infty$,
\[
\frac{1}{n}\E\left[\mathrm{trace}(\bfX_i\bfX_j)\right] - \frac{1}{n}\E\left[\mathrm{trace}(\bfX_i)\right]\frac{1}{n}\E\left[\mathrm{trace}(\bfX_j)\right] \rightarrow 0 \;.
\]
Other tools from random matrix theory such as transforms of spectral distributions are also used in their analysis, but we do not reach the stage in the analysis where they are useful in this report and so we do not cover them here. \todo


\subsection{Related work} \label{sec:literature}

Sketched algorithms for ridge regression have been previously considered in the literature. \citet{Chowdhury:2018} examined ridge regression in the setting where $n\ll d$ and showed that under certain conditions on the sketched approximation, the relative error in the solution of the partial Newton sketch algorithm is bounded above by an exponentially decaying tolerance. \citet{Wang:2017} compared the classical sketching algorithm (i.e., sketching both the data matrix and the response vector) to the partial Newton sketch algorithm in matrix ridge regression for the case $d\ll n$ and found that both have increased risks relative to the optimal solution. They proposed model averaging as a solution for improving the theoretical properties. It is notable that the analyses by \citet{Chowdhury:2018} and \citet{Wang:2017} are both based on conventional statistical learning techniques and that the specific sketching matrix used is not a focus of the analysis.
\\

In this project, we attempt to analyze the partial Newton sketch for ridge regression following the approach that \citet{Lacotte:2020} used for iterative Hessian sketch (IHS) with OLS. Their approach relies on asymptotic results from random matrix theory and free probability, and this appeared to allow a finer-grain analysis of IHS where the derived convergence rate depends on the specific sketching matrix used. The random matrix theoretic approach to analysis does not seem to have been considered much outside of OLS problems~\citep{Dobriban:2019,Lacotte:2020b}. As we explore in this project, a possible reason for this may be that even simple extensions to standard OLS make it difficult to directly apply existing results from these subfields of probability.


\subsection{Newton sketch for ridge regression} \label{sec:ridgesketch}

Consider the ridge regression loss function
\[
f(\bfb) = \frac{1}{2}\|\bfX\bfb-\bfy\|_2^2 + \frac{\lambda}{2}\|\bfb\|_2^2
\]
for $\bfb\in\bbR^d$ given data matrix $\bfX\in\bbR^{n\times d}$ with $d\ll n$, responses $\bfy\in\bbR^n$ and a regularization parameter $\lambda>0$. The gradient and Hessian of the function are respectively given by
\begin{align*}
\nabla f(\bfb) &= \left(\bfX^\T\bfX+\lambda\bfI_d\right)\bfb - \bfX^\T\bfy \;, \\
\bfH = \nabla^2f(\bfb) &= \bfX^\T\bfX+\lambda\bfI_d \;.
\end{align*}
The Newton updates described in Section~\ref{sec:newton} therefore have the form
\begin{align*}
\bfb_{t+1} &= \bfb_t - \alpha_t\bfH^{-1}\nabla f(\bfb_t) \\
&= \bfb_t - \alpha_t\left(\bfX^\T\bfX+\lambda\bfI_d\right)^{-1}\left(\left(\bfX^\T\bfX+\lambda\bfI_d\right)\bfb_t-\bfX^\T\bfy\right) \;.
\end{align*}
A partial Newton sketch of the Hessian has the form
\[
\bfH_t = \bfX^\T\bfS_t^\T\bfS_t\bfX + \lambda\bfI_d
\]
where $\bfS_t$ is a $m\times n$ refreshed sketching matrix with $d<m\ll n$. \todo:\citet{Chowdhury:2018} appendix results?. The partial Newton sketch updates for ridge regression are then given by
\[
\bfb_{t+1} = \bfb_t - \alpha_t\left(\bfX^\T\bfS_t^\T\bfS_t\bfX + \lambda\bfI_d\right)^{-1}\left(\left(\bfX^\T\bfX+\lambda\bfI_d\right)\bfb_t-\bfX^\T\bfy\right) \;.
\]
Note that \citet{Chowdhury:2018} and \citet{Wang:2017}) also considered this sketched update for ridge regression. However, our analysis approach differs from theirs in that we adopt the asymptotic random matrix theoretic approach from \citet{Lacotte:2020}. Also note that we do not consider updates with momentum as \citet{Lacotte:2020} did as we will show that extending their analysis approach from OLS to ridge regression is already non-trivial.


\subsection{Analysis attempt based on random matrix theory} \label{sec:theory}

In this section, we show that the proof technique used to obtain Theorems~3.1 and 4.1 of \citep{Lacotte:2020} do not easily generalize to the partial Newton sketch updates for ridge regression. We follow the general procedure of the proofs and show how far we can get with the ridge regression setup. We also highlight the key differences between OLS and ridge regression that leads to problems in the proof and discuss possible solutions for rectifying these problems in future work.
\\

The following conjecture formalizes the result analogous to Theorems~3.1 and 4.1 that we would like to prove. Note that additional assumptions will be added to the conjecture as we progress through the proof.

\begin{conjecture} \label{con:ridge}
Consider the partial Newton update for ridge regression described in Section~\ref{sec:ridgesketch}. Let $\bfX = \bfU\Sigma\bfV^\T$ be the thin singular value decomposition of $\bfX$ where $\bfU$ is a $n\times d$ semi-orthogonal matrix, $\bfV$ is a $d\times d$ orthogonal matrix, and $\Sigma$ is a $d\times d$ diagonal matrix with the singular values of $\bfX$ on the diagonal. Define the error vector $\Delta_t=\bfU^\T\bfX\left(\bfb_t-\bfb^*\right)$. For some optimal step size $\alpha_t$, the sequence of error vectors $\{\Delta_t\}$ satisfies
\[
\rho = \left(\lim_{n\rightarrow\infty}\frac{\E\left[\|\Delta_t\|_2^2\right]}{\|\Delta_0\|_2^2}\right)^\frac{1}{t}
\]
where $\rho$ is the optimal rate of convergence with some closed-form expression.
\end{conjecture}

We begin our attempt to prove Conjecture~\ref{con:ridge} following the proofs by \citet{Lacotte:2020}. Using the fact that the ridge regression solution satisfies the equation
\[
(\bfX^\T\bfX + \lambda\bfI_d)\bfb^* = \bfX^\T\bfY \;,
\]
the update can be rewritten as
\begin{align*}
\bfb_{t+1} &= \bfb_t - \alpha_t\left(\bfX^\T\bfS_t^\T\bfS_t\bfX + \lambda\bfI_d\right)^{-1}\left(\left(\bfX^\T\bfX+\lambda\bfI_d\right)\bfb_t-\left(\bfX^\T\bfX+\lambda\bfI_d\right)\bfb^*\right) \\
&= \bfb_t - \alpha_t\left(\bfX^\T\bfS_t^\T\bfS_t\bfX + \lambda\bfI_d\right)^{-1}\left(\bfX^\T\bfX+\lambda\bfI_d\right)\left(\bfb_t-\bfb^*\right) \;.
\end{align*}
Using the thin SVD of $\bfX$, we have the matrix identities
\begin{align*}
\bfX^\T\bfX+\lambda\bfI_d &= \bfV\Sigma^2\bfV^\T + \lambda \bfV\bfV^\T \\
&= \bfV\left(\Sigma^2+ \lambda\bfI_d\right)\bfV^\T \;, \\
\left(\bfX^\T\bfS_t^\T\bfS_t\bfX + \lambda\bfI_d\right)^{-1} &= \left(\bfV\Sigma\bfU^\T\bfS_t^\T\bfS_t\bfU\Sigma\bfV^\T + \lambda\bfV\bfV^\T\right)^{-1} \\
&= \bfV\left(\Sigma\bfU^\T\bfS_t^\T\bfS_t\bfU\Sigma + \lambda\bfI_d\right)^{-1}\bfV^\T \;.
\end{align*}
However, in order to later on obtain an expression in terms of $\Delta_t$ as in the original proof, we require that the data matrix be full column rank. This is a less than ideal assumption to make as one of the advantages of ridge regression is being able to obtain an unique solution with non-full rank data matrices. We return to this point in Section~\ref{sec:fullrank} to discuss how this assumption may be avoided.

\begin{assumption} \label{asp:rank}
The data matrix $\bfX$ has full column rank.
\end{assumption}

Under Assumption~\ref{asp:rank}, the singular values of $\bfX$ are non-zero and so the above matrices can be rewritten as
\begin{align*}
\bfX^\T\bfX+\lambda\bfI_d &= \bfV\Sigma\left(\bfI_d + \lambda\Sigma^{-2}\right)\Sigma\bfV^\T \;, \\
\left(\bfX^\T\bfS_t^\T\bfS_t\bfX + \lambda\bfI_d\right)^{-1} &= \bfV\Sigma^{-1}\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\Sigma^{-1}\bfV^\T \;.
\end{align*}
Replacing the corresponding matrices in the update with these identities gives
\[
\bfb_{t+1} = \bfb_t - \alpha_t\bfV\Sigma^{-1}\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2} \right)\Sigma\bfV^\T\left(\bfb_t-\bfb^*\right) \;.
\]
Multiplying both sides by $\bfU^\T\bfX$ gives
\begin{align*}
\bfU^\T\bfX\bfb_{t+1} &= \bfU^\T\bfX\bfb_t - \alpha_t\bfU^\T\bfX\bfV\Sigma^{-1}\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2} \right)\Sigma\bfV^\T\left(\bfb_t-\bfb^*\right) \\
&= \bfU^\T\bfX\bfb_t - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2} \right)\Sigma\bfV^\T\left(\bfb_t-\bfb^*\right)
\end{align*}
and then subtracting both sides by $\bfU^\T\bfX\bfb^*$ gives
\begin{align*}
\bfU^\T\bfX(\bfb_{t+1}-\bfb^*) &= \bfU^\T\bfX(\bfb_t-\bfb^*) - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2} \right)\Sigma\bfV^\T\left(\bfb_t-\bfb^*\right) \\
&= \left(\bfI_d - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2}\right)\right)\bfU^\T\bfX\left(\bfb_t-\bfb^*\right) \;.
\end{align*}
Let $\bfQ_t=\bfI_d - \alpha_t\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2}\right)$. Therefore by definition, we have $\Delta_{t+1} = \bfQ_t\Delta_t$ and
\[
\|\Delta_{t+1}\|^2 = \Delta_t^\T\bfQ_t^\T\bfQ_t\Delta_t \;.
\]
Taking the expectation with respect to $\bfS_t$, we get
\[
\E\left[\|\Delta_{t+1}\|^2\right] = \Delta_t^\T\E\left[\bfQ_t^\T\bfQ_t\right]\Delta_t
\]
where
\begin{align*}
\E\left[\bfQ_t^\T\bfQ_t\right] &= \bfI_d - \alpha_t\E\left[\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\right]\left(\bfI_d + \lambda\Sigma^{-2}\right) \\
&\quad - \alpha_t\left(\bfI_d + \lambda\Sigma^{-2}\right)\E\left[\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-1}\right] \\
&\quad + \alpha_t^2\left(\bfI_d + \lambda\Sigma^{-2}\right)\E\left[\left(\bfU^\T\bfS_t^\T\bfS_t\bfU + \lambda\Sigma^{-2}\right)^{-2}\right]\left(\bfI_d + \lambda\Sigma^{-2}\right) \;.
\end{align*}
At this point, we run into our first major obstacle that prevents us from applying the key step of the proof of Theorem~3.1. In Theorem~3.1 for OLS, the expression that is obtained from taking the expectation is
\[
\E\left[\|\Delta_{t+1}\|^2\right] = \Delta_t^\T\E\left[\bfR_t^2\right]\Delta_t
\]
where
\[
\E\left[\bfR_t^2\right] = \bfI_d - 2\alpha_t\E\left[\left(\bfU^\T\bfS_t^\T\bfS_t\bfU\right)^{-1}\right] + \alpha_t^2\E\left[\left(\bfU^\T\bfS_t^\T\bfS_t\bfU\right)^{-2}\right] \;.
\]
The proof of Theorem~3.1 proceeds to recognize that the matrix $\bfS_t\bfU$ can be embedded into a Haar matrix and is therefore rotationally invariant. Using exchangeability arguments, the expectations $\E\left[\left(\bfU^\T\bfS_t^\T\bfS_t\bfU\right)^{-p}\right]$ have a simple closed-form expression in terms of the inverse moments from which the rest of the proof follows. We do not have rotational invariance in our ridge regression case, and so we follow the proof of Theorem~4.1 from this point onwards. We require an additional assumption on the initialization of the problem.

\begin{assumption} \label{asp:initialization}
The initial error vector $\Delta_0$ is random, independent of $\bfS_0,\ldots,\bfS_t$, and satisfies $\E\left[\Delta_0\Delta_0^\T\right]=\frac{\bfI_d}{d}$.\footnote{The convergence rate $\rho$ in the statement of Conjecture~\ref{con:ridge} is also redefined as $\left(\lim_{n\rightarrow\infty}\frac{\E\left[\|\Delta_t\|_2^2\right]}{\E\left[\|\Delta_0\|_2^2\right]}\right)^\frac{1}{t}$ where $\E\left[\|\Delta_0\|_2^2\right]=\E\left[\Delta_0^\T\Delta_0\right]=\mathrm{trace}\left(\E\left[\Delta_0\Delta_0^\T\right]\right)=1$.}
\end{assumption}

Under Assumption~\ref{asp:initialization}, taking the expectation with respect to $\bfS_t$ instead gives
\begin{align*}
\E\left[\|\Delta_{t+1}\|^2\right] &= \E\left[\Delta_t^\T\bfQ_t^\T\bfQ_t\Delta_t\right] \\
&= \E\left[\Delta_0^\T\bfQ_0^\T\ldots \bfQ_t^\T\bfQ_t\ldots\bfQ_0\Delta_0\right] \\
&= \E\left[\mathrm{trace}\left(\Delta_0^\T\bfQ_0^\T\ldots \bfQ_t^\T\bfQ_t\ldots\bfQ_0\Delta_0\right)\right] \\
&= \mathrm{trace}\left(\E\left[\bfQ_0^\T\ldots \bfQ_t^\T\bfQ_t\ldots\bfQ_0\Delta_0\Delta_0^\T\right]\right) \;.
\end{align*}
By the independence of $\Delta_0$ and $\bfQ_i$, we then have
\begin{align*}
\E\left[\|\Delta_{t+1}\|^2\right] &= \mathrm{trace}\left(\E\left[\bfQ_0^\T\ldots \bfQ_t^\T\bfQ_t\ldots\bfQ_0\right]\E\left[\Delta_0\Delta_0^\T\right]\right) \\
&= \frac{1}{d}\mathrm{trace}\left(\E\left[\bfQ_0^\T\ldots \bfQ_t^\T\bfQ_t\ldots\bfQ_0\right]\right) \;.
\end{align*}
At this point in the proof of Theorem~4.1 for OLS, \citet{Lacotte:2020} use the asymptotic freeness of $\bfQ_i=\bfI_d-\alpha_i\bfU^\T\bfS_i^\T\bfS_i\bfU$ and $\bfQ_j$ for $i\neq j$ to obtain a closed-form expression for the trace. However, asymptotic freeness for $\bfQ_i=\bfI_d - \alpha_i\left(\bfU^\T\bfS_i^\T\bfS_i\bfU + \lambda\Sigma^{-2}\right)^{-1}\left(\bfI_d + \lambda\Sigma^{-2}\right)$ in our ridge regression case is no longer obvious as we have lost the convenience of working with (partial) Haar matrices. Due to time constraints on this project, we move forward under the hypothetical assumption that asymptotic freeness is retained in the ridge regression case. \textit{If $\bfQ_i$ were asymptotically free from $\bfQ_j$ for $i\neq j$}, then from recursive application we obtain
\begin{align*}
\lim_{n\rightarrow\infty}\E\left[\|\Delta_{t+1}\|^2\right] &= \lim_{n\rightarrow\infty}\frac{1}{d}\mathrm{trace}\left(\E\left[\bfQ_1^\T\ldots \bfQ_t^\T\bfQ_t\ldots\bfQ_0\bfQ_0^\T\right]\right) \\
&= \lim_{n\rightarrow\infty}\frac{1}{d}\mathrm{trace}\left(\E\left[\bfQ_2^\T\ldots \bfQ_t^\T\bfQ_t\ldots\bfQ_2\bfQ_1\bfQ_1^\T\right]\right)\lim_{n\rightarrow\infty}\frac{1}{d}\mathrm{trace}\left(\E\left[\bfQ_0\bfQ_0^\T\right]\right) \\
&= \prod_{i=0}^t\lim_{n\rightarrow\infty}\frac{1}{d}\mathrm{trace}\left(\E\left[\bfQ_i\bfQ_i^\T\right]\right) \\
&= \prod_{i=0}^t\lim_{n\rightarrow\infty}\frac{1}{d}\mathrm{trace}\left(\E\left[\bfQ_i^\T\bfQ_i\right]\right) \;.
\end{align*}
Using the expression for $\E\left[\bfQ_i^\T\bfQ_i\right]$ given earlier, we have
\begin{align*}
\lim_{n\rightarrow\infty}\frac{1}{d}\mathrm{trace}\left(\E\left[\bfQ_i^\T\bfQ_i\right]\right) &= 1 - \lim_{n\rightarrow\infty}\frac{2\alpha_i}{d}\mathrm{trace}\left(\E\left[\left(\bfU^\T\bfS_i^\T\bfS_i\bfU + \lambda\Sigma^{-2}\right)^{-1}\right]\left(\bfI_d + \lambda\Sigma^{-2}\right)\right) \\
&\quad + \lim_{n\rightarrow\infty}\frac{\alpha_i^2}{d}\mathrm{trace}\left(\E\left[\left(\bfU^\T\bfS_i^\T\bfS_i\bfU + \lambda\Sigma^{-2}\right)^{-2}\right]\left(\bfI_d + \lambda\Sigma^{-2}\right)^2\right) \;.
\end{align*}
Our final obstacle then comes from giving a closed-form expression for the limiting spectral distributions of the above matrices. Assuming that the singular values of $\bfX$ are bounded as $n\rightarrow\infty$, it should be clear that a limiting spectral distribution exists in cases when it exists for $\bfU^\T\bfS_i^\T\bfS_i\bfU$. If constant closed-form expressions of the limiting traces could be derived, then the next step would be to minimize $\lim_{n\rightarrow\infty}\E\left[\|\Delta_{t+1}\|^2\right] = \rho(\alpha)^{t+1}$ as a function of constant step size $\alpha$. Note that $\rho(\alpha)$ is quadratic and convex in $\alpha$. Furthermore, because $\bfQ_i^\T\bfQ_i$ is positive semidefinite, its trace is non-negative and therefore $\rho(\alpha)\geq0$. Thus, minimizing the limiting error norm is equivalent to minimizing $\rho(\alpha)$. Due to time constraints on this project, we leave this idea for future work but provide a brief discussion in Section~\ref{sec:invmoment} as to why calculating the inverse moments is more challenging compared to the OLS case.

\subsubsection{Full column rank assumption} \label{sec:fullrank}

\subsubsection{Inverse moments} \label{sec:invmoment}


\subsection{Empirical experiments} \label{sec:empirical}


\subsection{Discussion} \label{sec:conclusion}

\todo reliance on refreshed

